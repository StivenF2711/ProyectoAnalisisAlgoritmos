TY  - JOUR
T1  - State-of-the-art of transmission expansion planning: A survey from restructuring to renewable and distributed electricity markets
AU  - Gomes, Phillipe Vilaça
AU  - Saraiva, João Tomé
JO  - International Journal of Electrical Power & Energy Systems
VL  - 111
SP  - 411
EP  - 424
PY  - 2019
DA  - 2019/10/01/
SN  - 0142-0615
DO  - https://doi.org/10.1016/j.ijepes.2019.04.035
UR  - https://www.sciencedirect.com/science/article/pii/S014206151831888X
KW  - Heuristics
KW  - Optimization
KW  - Mathematical programming
KW  - Metaheuristic
KW  - Transmission expansion planning
AB  - Transmission Expansion Planning (TEP) problem aims at identifying when and where new equipment as transmission lines, cables and transformers should be inserted on the grid. The transmission upgrade capacity is motivated by several factors as meeting the increasing electricity demand, increasing the reliability of the system and providing non-discriminatory access to cheap generation for consumers. However, TEP problems have been changing over the years as the electrical system evolves. In this way, this paper provides a detailed historical analysis of the evolution of the TEP over the years and the prospects for this challenging task. Furthermore, this study presents an outline review of more than 140 recent articles about TEP problems, literature insights and identified gaps as a critical thinking in how new tools and approaches on TEP can contribute for the new era of renewable and distributed electricity markets.
ER  - 

TY  - JOUR
T1  - Study on large time-delay constant temperature control system based on TEC
AU  - SHAN, Jiang-dong
AU  - WU, Ge
AU  - TIAN, Xiao-jian
JO  - The Journal of China Universities of Posts and Telecommunications
VL  - 17
SP  - 32
EP  - 35
PY  - 2010
DA  - 2010/12/01/
SN  - 1005-8885
DO  - https://doi.org/10.1016/S1005-8885(09)60586-0
UR  - https://www.sciencedirect.com/science/article/pii/S1005888509605860
KW  - thermoelectric cooler
KW  - large time-delay control system
KW  - proportion integration differentiation (PID) control
KW  - constant temperature control
AB  - This paper designes a diminutive constant temperature control system based on thermoelectric cooler (TEC). Considering that the system is a large time-delay control system, the paper proposes a new method to determine the transfer function of the controlled system which gets the transfer function by doing nonlinear fitting of the step response of the controlled system. The characteristics of system model which is established by the method are basically same as the actual constant temperature control system. This method provides a new way of thinking to the design of large time-delay control system.
ER  - 

TY  - JOUR
T1  - A novel information gain based approach for classification and dimensionality reduction of hyperspectral images
AU  - Elmaizi, Asma
AU  - Nhaila, Hasna
AU  - Sarhrouni, Elkebir
AU  - Hammouch, Ahmed
AU  - Nacir, Chafik
JO  - Procedia Computer Science
VL  - 148
SP  - 126
EP  - 134
PY  - 2019
DA  - 2019/01/01/
T2  - THE SECOND INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2018
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2019.01.016
UR  - https://www.sciencedirect.com/science/article/pii/S187705091930016X
KW  - Hyperspectral images
KW  - dimentionality reduction
KW  - information gain
KW  - classification accuracy
AB  - Recently, the hyperspectral sensors has improved our ability to monitor the earth surface with high spectral resolution. However, the high dimensionality of spectral data brings challenges for the image processing. Consequently, the dimensionality reduction is a necessary step in order to reduce the computational complexity and increase the classification accuracy. In this paper, we propose a new filter approach based on information gain for dimensionality reduction and classification of hyperspectral images. A special strategy based on hyperspectral bands selection is adopted to pick the most informative bands and discard the irrelevant and noisy ones. The algorithm evaluates the relevancy of the bands based on the information gain function with the support vector machine classifier. The proposed method is compared using two benchmark hyperspectral datasets (Indiana, Pavia) with three competing methods. The comparison results showed that the information gain filter approach outperforms the other methods on the tested datasets and could significantly reduce the computation cost while improving the classification accuracy.
ER  - 

TY  - CHAP
T1  - Systematic Modeling for Optimization
AU  - Esche, Erik
AU  - Müller, David
AU  - Wozny, Günter
A2  - Eden, Mario R.
A2  - Siirola, John D.
A2  - Towler, Gavin P.
BT  - Computer Aided Chemical Engineering
PB  - Elsevier
VL  - 34
SP  - 699
EP  - 704
PY  - 2014
DA  - 2014/01/01/
T2  - Proceedings of the 8 International Conference on Foundations of Computer-Aided Process Design
SN  - 1570-7946
DO  - https://doi.org/10.1016/B978-0-444-63433-7.50101-2
UR  - https://www.sciencedirect.com/science/article/pii/B9780444634337501012
KW  - Multiple-Scale Modeling
KW  - Optimization
KW  - Convexification
KW  - Linearization
AB  - Optimization usually requires models, which are computationally speaking less expensive than models commonly used for simulations. At the same time, process optimization and model predictive control etc. require dependable accuracies in addition to the fastness. To demystify the art of preparing process models for optimization, a workflow is presented in this contribution, which systematically deduces models based on simplification of existing models and experiment based deduction of computationally inexpensive correlations.
ER  - 

TY  - JOUR
T1  - Induced worry increases risk aversion in patients with generalized anxiety
AU  - Sporrer, Juliana K.
AU  - Johann, Alexandra
AU  - Chumbley, Justin
AU  - Robinson, Oliver J.
AU  - Bach, Dominik R.
JO  - Behavioural Brain Research
VL  - 474
SP  - 115192
PY  - 2024
DA  - 2024/10/02/
SN  - 0166-4328
DO  - https://doi.org/10.1016/j.bbr.2024.115192
UR  - https://www.sciencedirect.com/science/article/pii/S0166432824003486
KW  - Depression
KW  - Anxiety
KW  - Worry
KW  - Decision making
KW  - Loss aversion
KW  - Risk aversion
AB  - Generalized anxiety disorder is characterized by disruptions in decision-making, including an enhanced aversion to uncertain outcomes (i.e., risk aversion), which is not specific to negative outcomes (i.e., no loss aversion). It is unknown if this uncertainty bias is a trait-like causal factor contributing to anxiety symptoms, or a state-like feature triggered by anxiety symptoms such as worry chains. Here, in-patients with Major Depression Disorder (MDD), with (N = 16) or without (N = 24) Generalized anxiety (GA) symptoms, and healthy controls (N = 23), completed an economic decision-making task before and after worry induction. They were asked to choose between a certain monetary payoff, and an uncertain gamble, allowing for estimation of risk and loss aversion through a computational prospect-theoretic model. There were no significant differences in risk and loss aversion between any of the three groups at baseline. After worry induction, patients with GA symptoms, compared to those without, showed increased risk aversion. This increase was modulated by the severity of anxiety symptoms. These findings suggest that decision-making disruptions in anxiety disorder may be driven by anxiety symptoms such as worry, rather than causing them. This could shape etiological models, motivate standardization of emotional state in research on decision-making in anxiety disorders, support treatment strategies primarily aimed at worry management, and could guide novel interventions focusing on uncertainty exposure across aversive and appetitive domains.
ER  - 

TY  - JOUR
T1  - A decision support engine for infill drilling attractiveness evaluation using rule-based cognitive computing under expert uncertainties
AU  - Mao, Qiangqiang
AU  - Ma, Xiaohua
AU  - Wang, Yuhe
JO  - Journal of Petroleum Science and Engineering
VL  - 208
SP  - 109671
PY  - 2022
DA  - 2022/01/01/
SN  - 0920-4105
DO  - https://doi.org/10.1016/j.petrol.2021.109671
UR  - https://www.sciencedirect.com/science/article/pii/S0920410521013000
KW  - Cognitive computing
KW  - Fuzzy inference
KW  - Infill well placement
KW  - Drilling attractiveness evaluation
KW  - Expert uncertainties quantification
AB  - Optimally drilling new wells in a developed reservoir is an essential strategy to potentially tap remaining oil for a complete life circle of oilfield development. Further, the determination of optimal infill drilling targets is a challenging issue which involves the integration of data, experts' knowledge and human decisions. The decision process can be essentially regarded as a systematic evaluation of drilling attractiveness. To automate drilling attractiveness evaluation, we develop a decision support engine using rule-based cognitive computing to rank and recommend drilling candidates. Such drilling candidates are chosen by the quantification of regional drilling attractiveness. Then we use two cases with different settings to show its general applicability and human-like reasoning abilities. The reasoning process considers expertise and human-involved uncertainties. The expertise is characterized by certain representation of fuzzy rules sets. Our results highlight the potential of our recommendation engine in pinpointing the most productive drilling location. And our method avoids the expensive reservoir simulation runs. Moreover, fuzzy drilling attractiveness evaluation can serve as an alternative initialization method of model-based infill well optimization, which avoids local optimum problem and greatly saves iteration time. Our approach extends human's reasoning capability and accelerates human's decision-making process with very low computational cost.
ER  - 

TY  - JOUR
T1  - Children's understanding of the inverse relation between multiplication and division
AU  - Robinson, Katherine M.
AU  - Dubé, Adam K.
JO  - Cognitive Development
VL  - 24
IS  - 3
SP  - 310
EP  - 321
PY  - 2009
DA  - 2009/07/01/
SN  - 0885-2014
DO  - https://doi.org/10.1016/j.cogdev.2008.11.001
UR  - https://www.sciencedirect.com/science/article/pii/S0885201408000889
KW  - Arithmetic
KW  - Inversion
KW  - Conceptual knowledge
KW  - Procedural knowledge
KW  - Factual knowledge
KW  - Multiplication
KW  - Division
AB  - Children's understanding of the inversion concept in multiplication and division problems (i.e., that on problems of the form d * e/e no calculations are required) was investigated. Children in Grades 6, 7, and 8 completed an inversion problem-solving task, an assessment of procedures task, and a factual knowledge task of simple multiplication and division. Application of the inversion concept in the problem-solving task was low and constant across grades. Most participants approved of the inversion-based shortcut but only a slight majority preferred it. Three clusters of children were identified based on their performance on the three tasks. The inversion cluster used and approved of the inversion shortcut the most and had high factual knowledge. The negation cluster used the negation strategy, had lower approval of the inversion shortcut, and had medium factual knowledge. The computation cluster used computation and had the lowest approval and the weakest factual knowledge. The findings highlight the importance of addressing the multiplication and division inversion concept in theories of children's mathematical competence.
ER  - 

TY  - JOUR
T1  - Position paper on the challenges posed by modern applications to cyber-physical systems theory
AU  - Allgöwer, Frank
AU  - Borges de Sousa, João
AU  - Kapinski, James
AU  - Mosterman, Pieter
AU  - Oehlerking, Jens
AU  - Panciatici, Patrick
AU  - Prandini, Maria
AU  - Rajhans, Akshay
AU  - Tabuada, Paulo
AU  - Wenzelburger, Philipp
JO  - Nonlinear Analysis: Hybrid Systems
VL  - 34
SP  - 147
EP  - 165
PY  - 2019
DA  - 2019/11/01/
SN  - 1751-570X
DO  - https://doi.org/10.1016/j.nahs.2019.05.007
UR  - https://www.sciencedirect.com/science/article/pii/S1751570X19300603
KW  - cyber–physical systems theory
AB  - Cyber-physical systems theory offers a powerful framework for modeling, analyzing, and designing real engineering systems integrating communication, control, and computation functionalities (the cyber part) within a natural and/or man-made system governed by the laws of physics (the physical part). New methodological developments in cyber-physical systems theory are required by traditional application domains such as manufacturing, transportation, and energy systems, which are currently experiencing significant and – to some extent – revolutionary changes to address the needs of our modern society. The goal of this position paper is to provide the cyber-physical systems community, and especially young researchers, a clear view on what are research directions worth pursuing motivated by the challenges posed by modern applications.
ER  - 

TY  - JOUR
T1  - FEFOS: a method to derive oxide formation energies from oxidation states††Electronic supplementary information (ESI) available. See DOI: https://doi.org/10.1039/d3cy00107e
AU  - Craig, Michael John
AU  - Kleuker, Felix
AU  - Bajdich, Michal
AU  - García-Melchor, Max
JO  - Catalysis Science & Technology
VL  - 13
IS  - 11
SP  - 3427
EP  - 3435
PY  - 2023
DA  - 2023/01/01/
SN  - 2044-4753
DO  - https://doi.org/10.1039/d3cy00107e
UR  - https://www.sciencedirect.com/science/article/pii/S2044475323006822
AB  - ABSTRACT
Herein we report a method to extract formation energies from oxidation states, which we call FEFOS. This new scheme predicts the formation energies of binary oxides through analyzing unary oxide formation energies as a function of their oxidation states. Taking averages of fitted quadratic equations that represent how elements respond to oxidation and reduction, the weights of these averages are determined by constraining the compound to be neutral. The application of FEFOS results in mean absolute errors of ca. 0.10 eV per atom when tested against Materials Project data for oxides with general formulas A1−zBzO, A1−zBzO1.5, and A1−zBzO2 with specific coordinations. Our FEFOS method not only allows for the prediction of binary oxide formation energies with low variance and high interpretability, but also compares well with state-of-the-art deep learning methods without being biased by training data and the need for large resources to compute it. Finally, we discuss the potential applications of the FEFOS method in tackling the problem of inverse catalyst design.
ER  - 

TY  - JOUR
T1  - Development of a system model to predict flows and performance of regional waste management planning: A case study of England
AU  - Ng, Kok Siew
AU  - Yang, Aidong
JO  - Journal of Environmental Management
VL  - 325
SP  - 116585
PY  - 2023
DA  - 2023/01/01/
SN  - 0301-4797
DO  - https://doi.org/10.1016/j.jenvman.2022.116585
UR  - https://www.sciencedirect.com/science/article/pii/S0301479722021582
KW  - Circular economy
KW  - Recycling
KW  - Stock-and-flow
KW  - Sustainable waste management
KW  - Resource recovery
KW  - Systems thinking
AB  - Significant loss of valuable resources and increasing burdens on landfills are often associated with a lack of proper planning in waste management and resource recovery strategy. A sustainable waste management model is thus urgently needed to improve resource efficiency and divert more waste from landfills. This paper proposes a comprehensive system model using stock-and-flow diagram to examine the current waste management performance and project the future waste generation, treatment and disposal scenarios, using England as a case study. The model comprises three integrated modules to represent household waste generation and collection; waste treatment and disposal; and energy recovery. A detailed mass and energy balance has been established and waste management performance has been evaluated using six upstream and downstream indicators. The base case scenario that assumes constant waste composition shows that waste to landfills can be reduced to less than 10% of the total amount, by 2035. However, it entails greater diversion of waste to energy-from-waste facilities, which is not sustainable and would incur higher capital investment and gate fees. Alternative case scenarios that promote recycling instead of energy recovery result in lower capital investment and gate fees. Complete elimination of the food and organic fraction from the residual waste stream will help meet the 65% recycling target by 2035. In light of the need for achieving a more circular economy in England, enhancing material recovery through reuse and recycling, reducing reliance on energy-from-waste and deploying more advanced waste valorisation technologies should be considered in future policy and planning for waste management.
ER  - 

TY  - JOUR
T1  - Introducing mindset streams to investigate stances towards STEM in high school students and experts
AU  - Brian, Kieran
AU  - Stella, Massimo
JO  - Physica A: Statistical Mechanics and its Applications
VL  - 626
SP  - 129074
PY  - 2023
DA  - 2023/09/15/
SN  - 0378-4371
DO  - https://doi.org/10.1016/j.physa.2023.129074
UR  - https://www.sciencedirect.com/science/article/pii/S0378437123006295
KW  - Cognitive modelling
KW  - Network science
KW  - Cognitive network science
KW  - Mindset reconstruction
KW  - STEM learning
AB  - We introduce mindset streams for assessing ways of bridging two target concepts in concept maps. We focus on behavioural forma mentis networks (BFMN), which map the associative and affective dimensions of memory recalls. Inspired by trains of thoughts taking several paths to link ideas, mindset streams are defined as BFMN subgraphs induced by all shortest paths between two target concepts, e.g. all recalls in shortest paths bridging “math” and “learning”. These streams quantify the following features of the mindset encoded in a BFMN: (i) semantic content (i.e. which ideas mediate connections between targets?), (ii) valence coherence/conflict (i.e. are connections mediated by entwining ideas perceived negatively, positively or neutrally?), and (iii) semantic relevance (i.e. are the bridges between targets peripheral or central for the connectivity/betweenness of the BFMN?). We investigate mindset streams between ‘maths”/“physics” and key motivational aspects of learning (“fun”, “work”, “failure”) in two BFMNs, encoding how 159 students and 59 experts perceived and associated concepts about Science Technology Engineering and Maths (STEM), respectively. Statistical comparisons against configuration models show that high schoolers bridge “maths” and “fun” only through overabundant levels of valence-conflicting associations, contrasting negatively perceived domain knowledge with peer-related positive experiences. This conflict is absent in the researchers’ mindset stream, which rather bridges “math” and “fun” through positive, science-related associations. The mindset streams of both groups bridge “maths” and “physics” to “work” through mostly positive career-related jargon. Students’ mindset streams of “failure” and “math”/“physics” are dominated by negative associations with test anxiety, whereas researchers integrate “failure” and “math”/“physics” in semantically richer and more positive contexts, denoting failure itself as a cornerstone of STEM learning. We discuss our findings and future research directions in view of relevant psychology/education literature.
ER  - 

TY  - JOUR
T1  - Attention-based Convolutional Autoencoders for 3D-Variational Data Assimilation
AU  - Mack, Julian
AU  - Arcucci, Rossella
AU  - Molina-Solana, Miguel
AU  - Guo, Yi-Ke
JO  - Computer Methods in Applied Mechanics and Engineering
VL  - 372
SP  - 113291
PY  - 2020
DA  - 2020/12/01/
SN  - 0045-7825
DO  - https://doi.org/10.1016/j.cma.2020.113291
UR  - https://www.sciencedirect.com/science/article/pii/S004578252030476X
KW  - Variational Data Assimilation
KW  - Attention networks
KW  - Convolutional Autoencoders
AB  - We propose a new ‘Bi-Reduced Space’ approach to solving 3D Variational Data Assimilation using Convolutional Autoencoders. We prove that our approach has the same solution as previous methods but has significantly lower computational complexity; in other words, we reduce the computational cost without affecting the data assimilation accuracy. We tested our proposal with data from a real-world application: a pollution model of a site in Elephant and Castle (London, UK) and found that we could (1) reduce the size of the background covariance matrix representation by O(103), and (2) increase our data assimilation accuracy with respect to existing reduced space methods.
ER  - 

TY  - JOUR
T1  - Science of science: A multidisciplinary field studying science
AU  - Krauss, Alexander
JO  - Heliyon
VL  - 10
IS  - 17
SP  - e36066
PY  - 2024
DA  - 2024/09/15/
SN  - 2405-8440
DO  - https://doi.org/10.1016/j.heliyon.2024.e36066
UR  - https://www.sciencedirect.com/science/article/pii/S240584402412097X
KW  - Science of science
KW  - Metascience
KW  - Foundations of science
KW  - Foundations of knowledge
KW  - Limits of science
KW  - Limits of knowledge
KW  - Origins of science
KW  - Origins of knowledge
AB  - Science and knowledge are studied by researchers across many disciplines, examining how they are developed, what their current boundaries are and how we can advance them. By integrating evidence across disparate disciplines, the holistic field of science of science can address these foundational questions. This field illustrates how science is shaped by many interconnected factors: the cognitive processes of scientists, the historical evolution of science, economic incentives, institutional influences, computational approaches, statistical, mathematical and instrumental foundations of scientific inference, scientometric measures, philosophical and ethical dimensions of scientific concepts, among other influences. Achieving a comprehensive overview of a multifaceted field like the science of science requires pulling together evidence from the many sub-fields studying science across the natural and social sciences and humanities. This enables developing an interdisciplinary perspective of scientific practice, a more holistic understanding of scientific processes and outcomes, and more nuanced perspectives to how scientific research is conducted, influenced and evolves. It enables leveraging the strengths of various disciplines to create a holistic view of the foundations of science. Different researchers study science from their own disciplinary perspective and use their own methods, and there is a large divide between quantitative and qualitative researchers as they commonly do not read or cite research using other methodological approaches. A broader, synthesizing paper employing a qualitative approach can however help provide a bridge between disciplines by pulling together aspects of science (economic, scientometric, psychological, philosophical etc.). Such an approach enables identifying, across the range of fields, the powerful role of our scientific methods and instruments in shaping most aspects of our knowledge and science, whereas economic, social and historical influences help shape what knowledge we pursue. A unifying theory is then outlined for science of science – the new-methods-drive-science theory.
ER  - 

TY  - JOUR
T1  - A deep learning-based edge-fog-cloud framework for driving behavior management
AU  - Al-Rakhami, Mabrook S.
AU  - Gumaei, Abdu
AU  - Hassan, Mohammad Mehedi
AU  - Alamri, Atif
AU  - Alhussein, Musaed
AU  - Razzaque, Md. Abdur
AU  - Fortino, Giancarlo
JO  - Computers & Electrical Engineering
VL  - 96
SP  - 107573
PY  - 2021
DA  - 2021/12/01/
SN  - 0045-7906
DO  - https://doi.org/10.1016/j.compeleceng.2021.107573
UR  - https://www.sciencedirect.com/science/article/pii/S0045790621005127
KW  - Deep learning
KW  - Car mobile edge (CME)
KW  - Fog and cloud computing
KW  - Aggressive driving behaviors
AB  - Among the various reasons behind vehicle accidents, drivers' aggressiveness and distractions play a significant role. Deep learning (DL) algorithms inside a car mobile edge (CME) have been used for driver monitoring and to perform automated decision-making processes. Training and retraining the DL models in resource-constrained CME devices come with several challenges, especially regarding computational and memory space costs. Moreover, training the DL models periodically on representative data nearest to CME without imposing communication overheads on the cloud improves the quality of service (QoS) parameters, such as memory demand, processing time, power consumption, and bandwidth. This paper investigates the deployment of a deep neural network (DNN) model on a cloud-fog-edge computing framework for aggressive driver behavior detection and monitoring. To reach this goal, our framework proposes utilizing effective systems and databases of sensor-based metrics and data, cost-effective wireless networks, cloud-and fog-edge computing technologies, and the Internet. Experimental results of the DNN model showed that the accuracy of detection is improved by 1.84% compared with the current related work without any pre-processing window on data points that come from bio-signal sensors. Moreover, the experimental results of the networking part prove the efficiency and effectiveness of the proposed framework.
ER  - 

TY  - JOUR
T1  - Renewable energy selection for net-zero energy communities: Life cycle based decision making under uncertainty
AU  - Karunathilake, Hirushie
AU  - Hewage, Kasun
AU  - Mérida, Walter
AU  - Sadiq, Rehan
JO  - Renewable Energy
VL  - 130
SP  - 558
EP  - 573
PY  - 2019
DA  - 2019/01/01/
SN  - 0960-1481
DO  - https://doi.org/10.1016/j.renene.2018.06.086
UR  - https://www.sciencedirect.com/science/article/pii/S0960148118307389
KW  - Multi-criteria decision making
KW  - Life cycle thinking
KW  - Fuzzy techniques
KW  - Renewable energy
KW  - Community energy system planning
AB  - Developing net-zero energy communities powered by renewable energy (RE) resources has become a popular concept. To make the best choices for community-level net-zero energy systems, it is necessary to identify the best energy technologies at local level. Evaluation of RE technologies has to be extended from technical and economic aspects to include environmental and social wellbeing. It is possible to identify the true costs and benefits of energy use by taking a cradle-to-grave life cycle perspective. In this study, a RE screening and multi-stage energy selection framework was developed. A fuzzy multi-criteria decision making approach was used in ranking the technologies to incorporate the conflicting requirements, stakeholder priorities, and uncertainties. Different scenarios were investigated to reflect different decision maker priorities. Under a pro-environment scenario, small hydro, onshore wind, and biomass combustion technologies perform best. Under a pro-economic decision scenario, biomass combustion, small hydro, and landfill gas have the best rankings. Triple bottom line sustainability was combined with technical feasibility through a ruled-based approach to avoid the theoretical pitfalls inherent in energy-related decision making. This assessment is geared towards providing decision makers with flexible tools, and is expected to aid in the pre-project planning stage of RE projects.
ER  - 

TY  - JOUR
T1  - Liability for damages caused by artificial intelligence
AU  - Čerka, Paulius
AU  - Grigienė, Jurgita
AU  - Sirbikytė, Gintarė
JO  - Computer Law & Security Review
VL  - 31
IS  - 3
SP  - 376
EP  - 389
PY  - 2015
DA  - 2015/06/01/
SN  - 0267-3649
DO  - https://doi.org/10.1016/j.clsr.2015.03.008
UR  - https://www.sciencedirect.com/science/article/pii/S026736491500062X
KW  - Artificial intelligence
KW  - Liability for damages
KW  - Legal regulation
KW  - AI-as-Tool
KW  - Risks by AI
KW  - Respondeat (respondent) superior
KW  - Vicarious liability
KW  - Strict liability
AB  - The emerging discipline of Artificial Intelligence (AI) has changed attitudes towards the intellect, which was long considered to be a feature exclusively belonging to biological beings, i.e. homo sapiens. In 1956, when the concept of Artificial Intelligence emerged, discussions began about whether the intellect may be more than an inherent feature of a biological being, i.e. whether it can be artificially created. AI can be defined on the basis of the factor of a thinking human being and in terms of a rational behavior: (i) systems that think and act like a human being; (ii) systems that think and act rationally. These factors demonstrate that AI is different from conventional computer algorithms. These are systems that are able to train themselves (store their personal experience). This unique feature enables AI to act differently in the same situations, depending on the actions previously performed. The ability to accumulate experience and learn from it, as well as the ability to act independently and make individual decisions, creates preconditions for damage. Factors leading to the occurrence of damage identified in the article confirm that the operation of AI is based on the pursuit of goals. This means that with its actions AI may cause damage for one reason or another; and thus issues of compensation will have to be addressed in accordance with the existing legal provisions. The main issue is that neither national nor international law recognizes AI as a subject of law, which means that AI cannot be held personally liable for the damage it causes. In view of the foregoing, a question naturally arises: who is responsible for the damage caused by the actions of Artificial Intelligence? In the absence of direct legal regulation of AI, we can apply article 12 of United Nations Convention on the Use of Electronic Communications in International Contracts, which states that a person (whether a natural person or a legal entity) on whose behalf a computer was programmed should ultimately be responsible for any message generated by the machine. Such an interpretation complies with a general rule that the principal of a tool is responsible for the results obtained by the use of that tool since the tool has no independent volition of its own. So the concept of AI-as-Tool arises in the context of AI liability issues, which means that in some cases vicarious and strict liability is applicable for AI actions.
ER  - 

TY  - JOUR
T1  - Poor adherence is a major barrier to the proper treatment of cutaneous leishmaniasis: A case-control field assessment in Iran
AU  - Bamorovat, Mehdi
AU  - Sharifi, Iraj
AU  - Agha Kuchak Afshari, Setareh
AU  - Karamoozian, Ali
AU  - Tahmouresi, Amirhossein
AU  - Heshmatkhah, Amireh
AU  - Salarkia, Ehsan
AU  - Khosravi, Ahmad
AU  - Hakimi Parizi, Maryam
AU  - Barghi, Maryam
JO  - International Journal for Parasitology: Drugs and Drug Resistance
VL  - 21
SP  - 21
EP  - 27
PY  - 2023
DA  - 2023/04/01/
SN  - 2211-3207
DO  - https://doi.org/10.1016/j.ijpddr.2022.11.006
UR  - https://www.sciencedirect.com/science/article/pii/S2211320722000331
KW  - Poor adherence
KW  - Cutaneous leishmaniasis
KW  - Major barrier
KW  - Treatment
KW  - Iran
AB  - Leishmaniasis is an overlooked, poverty-stricken, and complex disease with growing social and public health problems. In general, leishmaniasis is a curable disease; however, there is an expansion of unresponsive cases to treatment in cutaneous leishmaniasis (CL). One of the effective and ignored determinants in the treatment outcome of CL is poor treatment adherence (PTA). PTA is an overlooked and widespread phenomenon to proper Leishmania treatment. This study aimed to explore the effect of poor adherence in unresponsiveness to treatment in patients with anthroponotic CL (ACL) by comparing conventional statistical modalities and machine learning analyses in Iran. Overall, 190 cases consisting of 50 unresponsive patients (case group), and 140 responsive patients (control group) with ACL were randomly selected. The data collecting form that included 25 queries (Q) was recorded for each case and analyzed by R software and genetic algorithm (GA) approaches. Complex treatment regimens (Q11), cultural and lay views about the disease and therapy (Q8), life stress, hopelessness and negative feelings (Q22), adverse effects of treatment (Q13), and long duration of the lesion (Q12) were the most prevalent significant variables that inhibited effective treatment adherence by the two methods, in decreasing order of significance. In the inherent algorithm approach, similar to the statistical approach, the most significant feature was complex treatment regimens (Q11). Providing essential knowledge about ACL and treatment of patients with chronic diseases and patients with misconceptions about chemical drugs are important issues directly related to the disease's unresponsiveness. Furthermore, early detection of patients to prevent the long duration of the disease and the process of treatment, efforts to minimize side effects of treatment, induction of positive thinking, and giving hope to patients with stress and anxiety by medical staff, and family can help patients adhere to the treatment.
ER  - 

TY  - JOUR
T1  - Moment-matching approximations for stochastic sums in non-Gaussian Ornstein–Uhlenbeck models
AU  - Brignone, Riccardo
AU  - Kyriakou, Ioannis
AU  - Fusai, Gianluca
JO  - Insurance: Mathematics and Economics
VL  - 96
SP  - 232
EP  - 247
PY  - 2021
DA  - 2021/01/01/
SN  - 0167-6687
DO  - https://doi.org/10.1016/j.insmatheco.2020.12.002
UR  - https://www.sciencedirect.com/science/article/pii/S0167668720301645
KW  - Mean reversion
KW  - Non-Gaussian processes
KW  - Moment-matching
KW  - Asian option valuation
KW  - Stochastic annuities
AB  - In this paper, we recall actuarial and financial applications of sums of dependent random variables that follow a non-Gaussian mean-reverting process and contemplate distribution approximations. Our work complements previous related studies restricted to lognormal random variables; we revisit previous approximations and suggest new ones. We then derive moment-based distribution approximations for random sums attuned to Asian option pricing and computation of risk measures of random annuities. Various numerical experiments highlight the speed–accuracy benefits of the proposed methods.
ER  - 

TY  - JOUR
T1  - Cross-MapReduce: Data transfer reduction in geo-distributed MapReduce
AU  - Marzuni, Saeed Mirpour
AU  - Savadi, Abdorreza
AU  - Toosi, Adel N.
AU  - Naghibzadeh, Mahmoud
JO  - Future Generation Computer Systems
VL  - 115
SP  - 188
EP  - 200
PY  - 2021
DA  - 2021/02/01/
SN  - 0167-739X
DO  - https://doi.org/10.1016/j.future.2020.09.009
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X20305847
KW  - MapReduce
KW  - Geo-distributed
KW  - Data center
KW  - Big data
AB  - The MapReduce model is widely used to store and process big data in a distributed manner. MapReduce was originally developed for a single tightly coupled cluster of computers. Approaches such as Hierarchical and Geo-Hadoop are designed to address geo-distributed MapReduce processing. However, these methods still suffer from high inter-cluster data transfer over the Internet, which is prohibitive for processing today’s globally big data. In line with our thinking that there is no need to transfer the entire intermediate results to a single global reducer, we propose Cross-MapReduce, a framework for geo-distributed MapReduce processing. Before any massive data transfer, our proposed method finds a set of best global reducers to minimize transferred data volumes. We propose a graph called Global Reduction Graph (GRG) to determine the number and the locations of the global reducers. We conducted extensive experimental evaluations using a real testbed to demonstrate the effectiveness of Cross-MapReduce. The experimental results show that Cross-MapReduce significantly outperforms the Hierarchical and Geo-Hadoop approaches and reduces the amount of data transfer over the Internet by 40%.
ER  - 

TY  - JOUR
T1  - Artificial states? On the enduring geographical myth of natural borders
AU  - Fall, Juliet J.
JO  - Political Geography
VL  - 29
IS  - 3
SP  - 140
EP  - 147
PY  - 2010
DA  - 2010/03/01/
SN  - 0962-6298
DO  - https://doi.org/10.1016/j.polgeo.2010.02.007
UR  - https://www.sciencedirect.com/science/article/pii/S0962629810000533
KW  - Artificial states
KW  - Boundaries
KW  - Ethnic homogeneity
KW  - Failed states
KW  - Nationalism
KW  - Natural boundaries
KW  - Territorial trap
AB  - Alberto Alesina, William Easterly and Janina Matuszeski's paper Artificial States, published as a National Bureau of Economic Research Working Paper in June 2006, suggests a theory linking the nature of country borders to the economic success of countries (Alesina, Easterly, & Matuszeski, 2006). This paper critically examines this suggestion that natural boundaries and ethnic homogeneity are desirable for economic reasons. It takes issue with the understanding of artificial and natural boundaries that they develop, arguing that this ignores two centuries of critical and quantitative geographical scholarship that has mapped, documented and critiqued the obsession of a link between topography and the appropriate shape of states and boundaries. It explores how their argument is linked to a defence of ethnically homogeneous states. The focus is on their teleological and paradoxically ahistorical vision that naturalizes politics by appealing to spatial myths of homogeneity and geometric destiny, grounded in a reactionary understanding of space as container. In so doing, I am mindful of the strong links between such proposals and calls for post-conflict partition, and the corresponding discourses of ethnic and cultural homogenization on which they rely. Instead of thinking of boundaries as geometric objects, squiggly or not, I consider boundaries through the simultaneous processes of reification, naturalization, and fetishization.
ER  - 

TY  - JOUR
T1  - An experimental and simulation study of the impact of emotional information on analogical reasoning
AU  - Castro, Ariana A.
AU  - Hummel, John E.
AU  - Berenbaum, Howard
JO  - Cognition
VL  - 238
SP  - 105510
PY  - 2023
DA  - 2023/09/01/
SN  - 0010-0277
DO  - https://doi.org/10.1016/j.cognition.2023.105510
UR  - https://www.sciencedirect.com/science/article/pii/S0010027723001440
KW  - Reasoning
KW  - Emotion
KW  - Computational models
KW  - Attention
KW  - Analogies
AB  - We investigated whether and how emotional information would affect analogical reasoning. We hypothesized that task-irrelevant emotional information would impair performance whereas task-relevant emotional information would enhance it. In Study 1, 233 undergraduates completed a novel version of the People Pieces Task (Emotional Faces People Task), an analogical reasoning task in which the task characters displayed emotional or neutral facial expressions (within-participants). The emotional faces were relevant or irrelevant to the task (between-participants). We simulated the behavioral results using the Learning and Inference with Schemas and Analogies (LISA) model of relational reasoning. LISA is a neurally plausible, symbolic-connectionist computational model of analogical reasoning. In comparison to neutral trials, participants were slower but more accurate on emotion-relevant trials, and were faster but less accurate on emotion-irrelevant trials. Simulations using the LISA model demonstrated that it is possible to account for the effects of emotional information on reasoning in terms of how emotional stimuli attract attention during a reasoning task. In Study 2, 255 undergraduates completed the Emotional Faces People Task at either a high- or low-working memory load. The high working memory load condition of Study 2 replicated the findings of Study 1, showing that participants were more accurate on emotion-relevant trials than on emotion-irrelevant trials; in Study 2, this increased accuracy could not be accounted for by a speed-accuracy tradeoff. The working memory manipulation influenced the manner in which the congruence (with the correct answer) of emotion-irrelevant emotion influenced performance. Simulations using the LISA model showed that manipulating the salience of emotion, the error penalty, as well as vigilance (which determines the likelihood that LISA will notice it has attended to an irrelevant relation), could reasonably reproduce the behavioral results of both low and high working memory load conditions of Study 2.
ER  - 

TY  - JOUR
T1  - Features for the 0-1 knapsack problem based on inclusionwise maximal solutions
AU  - Jooken, Jorik
AU  - Leyman, Pieter
AU  - De Causmaecker, Patrick
JO  - European Journal of Operational Research
VL  - 311
IS  - 1
SP  - 36
EP  - 55
PY  - 2023
DA  - 2023/11/16/
SN  - 0377-2217
DO  - https://doi.org/10.1016/j.ejor.2023.04.023
UR  - https://www.sciencedirect.com/science/article/pii/S0377221723003065
KW  - Combinatorial optimization
KW  - 0-1 knapsack problem
KW  - Packing
KW  - Problem instance hardness
KW  - Instance space analysis
AB  - Decades of research on the 0-1 knapsack problem led to very efficient algorithms that are able to quickly solve large problem instances to optimality. This prompted researchers to also investigate the structure of problem instances that are hard for existing solvers. In the current paper we are interested in investigating which features make 0-1 knapsack problem instances hard to solve to optimality for the state-of-the-art 0-1 knapsack solver. We propose a set of 14 features based on previous work by the authors in which so-called inclusionwise maximal solutions (IMSs) play a central role. Calculating these features is computationally expensive and requires one to solve hard combinatorial problems. Based on new structural results about IMSs, we formulate polynomial and pseudopolynomial time algorithms for calculating these features. These algorithms were executed for two large datasets on a supercomputer in approximately 540 CPU-hours. We show that the proposed features contain important information related to the empirical hardness of a problem instance that was missing in earlier features from the literature by training machine learning models that can accurately predict the empirical hardness of a wide variety of 0-1 knapsack problem instances. Moreover, we show that these features can be cheaply approximated at the cost of less accurate hardness predictions. Using the instance space analysis methodology, we show that hard 0-1 knapsack problem instances are clustered together around a relatively dense region of the instance space and several features behave differently in the easy and hard parts of the instance space.
ER  - 

TY  - CHAP
T1  - Geohydrology: Hydrological Modeling
AU  - Ogden, Fred L.
A2  - Alderton, David
A2  - Elias, Scott A.
BT  - Encyclopedia of Geology (Second Edition)
PB  - Academic Press
CY  - Oxford
SP  - 457
EP  - 476
PY  - 2021
DA  - 2021/01/01/
SN  - 978-0-08-102909-1
DO  - https://doi.org/10.1016/B978-0-08-102908-4.00115-6
UR  - https://www.sciencedirect.com/science/article/pii/B9780081029084001156
KW  - Conceptual
KW  - Data driven
KW  - Discretization
KW  - Heterogeneity
KW  - Machine learning
KW  - Perceptual
KW  - Physical
KW  - Process-based
KW  - Stochastic
KW  - Uncertainty
AB  - Hydrologic models simulate one or more components of the hydrological cycle, the global water cycle on Earth. This article discusses the features, constraints, and limitations of different broad classes of hydrologic models, along with challenges associated with their application. Heterogeneity and uncertainties in material properties dominate most hydrologic settings in nature. These factors make dominant flow paths and residence times highly uncertain in most settings. For this reason hydrologic models often begin from a perceptual model that the modeler believes to represent the important system behaviors. Next, a set of equations are coded into a computational model and tested. The most common computational hydrologic model types are: analytical, conceptual, data-driven, and process-based. All hydrologic models require use of a discretization, and are lumped at some scale. Purely physics-based models are possible only in rare special situations with low uncertainty in media properties and reduced heterogeneity.
ER  - 

TY  - JOUR
T1  - Reverse vaccinology approach for the identifications of potential vaccine candidates against Salmonella
AU  - Li, Jie
AU  - Qiu, Jingxuan
AU  - Huang, Zhiqiang
AU  - Liu, Tao
AU  - Pan, Jing
AU  - Zhang, Qi
AU  - Liu, Qing
JO  - International Journal of Medical Microbiology
VL  - 311
IS  - 5
SP  - 151508
PY  - 2021
DA  - 2021/07/01/
SN  - 1438-4221
DO  - https://doi.org/10.1016/j.ijmm.2021.151508
UR  - https://www.sciencedirect.com/science/article/pii/S1438422121000370
KW  - 
KW  - Reverse vaccinology
KW  - Computational model
KW  - Vaccine target
KW  - Immunoprotective
AB  - Salmonella is a leading cause of foodborne pathogen which causes intestinal and systemic diseases across the world. Vaccination is the most effective protection against Salmonella, but the identification and design of an effective broad-spectrum vaccine is still a great challenge, because of the multi-serotypes of Salmonella. Reverse vaccinology is a new tool to discovery and design vaccine antigens combining human immunology, structural biology and computational biology with microbial genomics. In this study, reverse vaccinology, an in-silico approach was established to screen appropriate immunogen targets by calculating the immunogenicity score of 583 non-redundant outer membrane and secreted proteins of Salmonella. Herein among 100 proteins identified with top-ranked scores, 15 representative antigens were selected randomly. Applying the sequence conservation test, four proteins (FliK, BcsZ, FhuA and FepA) remained as potential vaccine candidates for in vivo evaluation of immunogenicity and immunoprotection. All four candidates were capable to trigger the immune response and stimulate the production of antiserum in mice. Furthermore, top-ranked proteins including FliK and BcsZ provided wide antigenic coverage among the multi-serotype of Salmonella. The S. Typhimurium LT2 challenge model used in mice immunized with FliK and BcsZ showed a high relative percentage survival (RPS) of 52.74 % and 64.71 % respectively. In conclusion, this study constructed an in-silico pipeline able to successfully pre-screen the vaccine targets characterized by high immunogenicity and protective immunity. We show that reverse vaccinology allowed screening of appropriate broad-spectrum vaccines for Salmonella.
ER  - 

TY  - JOUR
T1  - Integrated nexus approach to assessing climate change impacts on grassland ecosystem dynamics: A case study of the grasslands in Tanzania
AU  - Zarei, Azin
AU  - Madani, Kaveh
AU  - Guenther, Edeltraud
AU  - Nasrabadi, Hamid Mohammadi
AU  - Hoff, Holger
JO  - Science of The Total Environment
VL  - 952
SP  - 175691
PY  - 2024
DA  - 2024/11/20/
SN  - 0048-9697
DO  - https://doi.org/10.1016/j.scitotenv.2024.175691
UR  - https://www.sciencedirect.com/science/article/pii/S0048969724058479
KW  - Climate change impacts
KW  - Emission scenarios
KW  - Vegetation dynamics
KW  - Nexus approach
KW  - Vulnerability
AB  - This study addresses the intricate interplay between climate, vegetation, and livestock dynamics in Tanzania within the Climate-Vegetation-Livestock (CVL) nexus through a quantitative assessment. By examining the temporal and spatial relationships between vegetation indices (NDVI, EVI, NPP) and key climatic variables (Precipitation, Temperature, Evapotranspiration) from 2009 to 2019, and projecting to 2050, this research aims to elucidate vegetation responses to climate change and its subsequent impacts on livestock. To this end, the relationship between the vegetation dynamics indicators (NDVI, NPP) and climate parameters is evaluated to quantify the vegetation response to climate change using statistical models. Next, an examination of multicollinearity is conducted to investigate potential interactions (nexus) between variables, incorporating the correlation among independent variables. Notably, the evaluation of performance and accuracy for the mentioned models is conducted through the cross-validation method and validation indices. Ultimately, the variation between projected NPP and NDVI (average for 2040–2060) and the present NPP and NDVI (average for 2009–2020) identifies the regions that are most likely susceptible, showcasing the vegetation cover's reaction to climate change in different emission scenarios. The results unveil significant spatio-temporal variations in vegetation dynamics influenced by climatic factors, where higher precipitation and temperatures correlate with increased vegetation health and productivity. The projected fluctuations in NDVI and NPP values indicate varying trends across different regions, with a general decrease in vegetation density and productivity from the northeast to the west under both RCP2.6 and RCP8.5 scenarios by 2050. This decline is attributed to anticipated changes in precipitation and temperature patterns driven by climate change. Furthermore, significant declines in vegetation density and productivity under emission scenarios, particularly in the southern regions compared to the present, suggest greater vulnerability to climate change impacts. This highlights the need for targeted mitigation strategies in these vulnerable areas. Meanwhile, northeast areas under both NDVI and NPP will remain unchanged across both climate scenarios. Moreover, analysis of livestock distribution maps indicates areas of vulnerability under climate change scenarios, with implications for future livestock management and agricultural practices. These findings underscore the importance of proactive planning and targeted interventions to enhance resilience and sustainable development in vulnerable regions, emphasizing the need for integrated approaches that consider the complex interactions between climate, vegetation, and livestock dynamics.
ER  - 

TY  - JOUR
T1  - Understanding psychiatric disorder by capturing ecologically relevant features of learning and decision-making
AU  - Scholl, Jacqueline
AU  - Klein-Flügge, Miriam
JO  - Behavioural Brain Research
VL  - 355
SP  - 56
EP  - 75
PY  - 2018
DA  - 2018/12/14/
T2  - SI: MCC 2016
SN  - 0166-4328
DO  - https://doi.org/10.1016/j.bbr.2017.09.050
UR  - https://www.sciencedirect.com/science/article/pii/S0166432817305673
KW  - Reinforcement learning
KW  - Decision-making
KW  - Computational psychiatry
AB  - Recent research in cognitive neuroscience has begun to uncover the processes underlying increasingly complex voluntary behaviours, including learning and decision-making. Partly this success has been possible by progressing from simple experimental tasks to paradigms that incorporate more ecological features. More specifically, the premise is that to understand cognitions and brain functions relevant for real life, we need to introduce some of the ecological challenges that we have evolved to solve. This often entails an increase in task complexity, which can be managed by using computational models to help parse complex behaviours into specific component mechanisms. Here we propose that using computational models with tasks that capture ecologically relevant learning and decision-making processes may provide a critical advantage for capturing the mechanisms underlying symptoms of disorders in psychiatry. As a result, it may help develop mechanistic approaches towards diagnosis and treatment. We begin this review by mapping out the basic concepts and models of learning and decision-making. We then move on to consider specific challenges that emerge in realistic environments and describe how they can be captured by tasks. These include changes of context, uncertainty, reflexive/emotional biases, cost-benefit decision-making, and balancing exploration and exploitation. Where appropriate we highlight future or current links to psychiatry. We particularly draw examples from research on clinical depression, a disorder that greatly compromises motivated behaviours in real-life, but where simpler paradigms have yielded mixed results. Finally, we highlight several paradigms that could be used to help provide new insights into the mechanisms of psychiatric disorders.
ER  - 

TY  - JOUR
T1  - Surface profile evolution model for titanium alloy machined using abrasive waterjet
AU  - Yuan, Yemin
AU  - Chen, Jianfeng
AU  - Gao, Hang
JO  - International Journal of Mechanical Sciences
VL  - 240
SP  - 107911
PY  - 2023
DA  - 2023/02/15/
SN  - 0020-7403
DO  - https://doi.org/10.1016/j.ijmecsci.2022.107911
UR  - https://www.sciencedirect.com/science/article/pii/S0020740322007895
KW  - Abrasive waterjet
KW  - Ti-6Al-4V alloy
KW  - Computational fluid dynamics (CFD)
KW  - Stagnation zone
KW  - Surface profile evolution
AB  - The surface profile evolution model, which was initially developed for glass and polymers, can accurately predict a channel profile cross-section produced by abrasive jet (AJ) machining. In this study, the model is modified and applied for estimating the profiles of a Ti-6Al-4V alloy eroded by an abrasive waterjet (AWJ). First, the velocity and mass fraction distributions of the gas–liquid–solid phases in the AWJ at the nozzle exit were derived and compared, and several improvements were proposed, such as considering the divergence angle of the jet and particles, as well as the length of the jet core area, to precisely construct a theoretical connection of the erosion efficiency distribution before impacting the workpiece. Computational fluid dynamics (CFD) simulations were then performed to investigate the behaviour of the erosion jets during surface evolution. The results revealed that the jet diffusion provoked by the stagnation zone effect became more pronounced as the surface profile depth deepened, which led to jet directional deflection and suppressed the erosion capacities of the AWJ. Therefore, a central erosion depth function was introduced to correct this detrimental effect with the intention of obtaining an accurate channel profile. In addition, a second-order single-step fitting function was suggested to eliminate the fluctuations caused by uneven abrasive particles and the problem of reduced erosion efficiency due to channel depth variation. Finally, based on the determination of the parameters affecting the channel profile, a normalised centre erosion rate function, which only depends on the channel depth and is isolated from the material properties and the standoff distance, was recommended to simplify the calculation. The erosion function conforming to a Gaussian surface was fitted using MATLAB (R2019b, MathWorks, USA). The results demonstrated that the channel profiles predicted by the surface evolution model were consistent with the measured profiles, with an average error of 11.4%.
ER  - 

TY  - JOUR
T1  - Effects of human land use and temperature on community dynamics in European forests
AU  - Milligan, G.
AU  - Bradshaw, R.H.W.
AU  - Clancy, D.
AU  - Żychaluk, K.
AU  - Spencer, M.
JO  - Quaternary Science Reviews
VL  - 247
SP  - 106458
PY  - 2020
DA  - 2020/11/01/
SN  - 0277-3791
DO  - https://doi.org/10.1016/j.quascirev.2020.106458
UR  - https://www.sciencedirect.com/science/article/pii/S0277379120304200
KW  - Holocene
KW  - Europe
KW  - Vegetation dynamics
KW  - Paleoclimatology
KW  - Compositional data analysis
KW  - Stochastic differential equations
KW  - Land use
AB  - Climate change and human land use are thought to play a dominant role in the dynamics of European central-latitude forests in the Holocene. A wide range of mathematical and statistical models have been used to study the effects of these variables on forest dynamics, including physiologically-based simulations and phenomenological community models. However, for statistical analysis of pollen count data, compositional data analysis is particularly well suited, because pollen counts give only relative information. We studied the effects of changes in human land use and temperature on European central-latitude forest dynamics at 7 sites over most of the last 10ka, using a stochastic model for compositional dynamics of pollen count data. Our approach has a natural ecological interpretation in terms of relative proportional population growth rates, and does not require information on pollen production, dispersal, or deposition. We showed that the relative proportional population growth rates of Fagus and Picea were positively affected by intensified human land use, and that those of Tilia and Ulmus were negatively affected. Also, the relative proportional population growth rate of Fagus was negatively affected by increases in temperature above about 18∘C. Overall, the effects of temperature on the rate of change of forest composition were more important than those of human land use. Although there were aspects of dynamics, such as short-term oscillations, that our model did not capture, our approach is broadly applicable and founded on ecological principles, and gave results consistent with current thinking.
ER  - 

TY  - JOUR
T1  - Brain science: On the way to solving the problem of consciousness
AU  - Ivanitsky, Alexey M.
AU  - Ivanitsky, George A.
AU  - Sysoeva, Olga V.
JO  - International Journal of Psychophysiology
VL  - 73
IS  - 2
SP  - 101
EP  - 108
PY  - 2009
DA  - 2009/08/01/
T2  - Neural Processes in Clinical Psychophysiology
SN  - 0167-8760
DO  - https://doi.org/10.1016/j.ijpsycho.2009.02.004
UR  - https://www.sciencedirect.com/science/article/pii/S0167876009001044
KW  - Consciousness and brain problem
KW  - Event-related potentials
KW  - EEG rhythms
KW  - Semantic brain systems
KW  - Artificial intelligence
AB  - Four issues are discussed: the possible mechanism of subjective events, conscious versus unconscious brain functions, the rhythmic coding of mental operations and the possible brain basis of understanding.i.Several approaches have been developed to explain how subjective experience emerges from brain activity. One of them is the return of the nervous impulses to the sites of their primary projections, providing a synthesis of sensory information with memory and motivation [Ivanitsky, A.M., 1976. Brain Mechanisms of the Signal Evaluation. Medicina, Moscow 264 pp. (in Russian)]. Support for the existence of such a mechanism stems from studies upon the brain activity that subserves perception (visual and somato-sensory) and thought (verbal and imaginative). The cortical centres for information synthesis have been found. For perception, these are located in projection areas; for thinking — in frontal and temporal-parietal associative cortex. Closely related ideas were also developed by G. Edelman [Edelman, G.M., 1978. Group selection and phasic reentrant signaling: A theory of higher brain function. In: Eds. Edelman, G.M., Mountcastle, V.B. The Mindful Brain. Cortical Organization and the Group-selective Theory of Higher Brain Function. Cambridge, MA, MIT Press, pp 51–100.] in his re-entry theory of consciousness. Both theories emphasize the key role of memory and motivation in the origin of conscious function.ii.Conscious experience elucidates not all, but only salient brain functions. As a rule, voluntary control is switched on when additional cognitive resources are needed. Even a rather complicated mental operation, such as the discrimination between concrete and abstract words, could be executed very rapidly and implicitly; explicit analysis being engaged only in more difficult tasks. Furthermore, these two different kinds of mental operations, i.e., automatic and conscious, are predominantly associated with two different kinds of memory: a recognition memory for implicit analysis, and an episodic memory for explicit functions.iii.Rearrangements of EEG rhythms underlie mental functions. Certain rhythmical patterns are related with definite types of mental activity. The dependence of one upon the other is rather pronounced and expressive, so it becomes possible to recognize the type of mental operation being performed in mind with few seconds of the ongoing EEG, provided that the analysis of rhythms is accomplished using an artificial neural network.iv.It is commonly recognized that the computer, in contrast to the living brain, can calculate, yet cannot understand [Penrose, R., 1996. Shadows of the Mind: A Search for the Missing Science of Consciousness New York, Oxford, Oxford University Press 480 pp.]. Comprehension implies the comparison of new and old information that requires the ability to search for associations, grouping similar objects together, and distinguishing different objects from one another. However, these functions may also be implemented on a computer. Still, it is believed that computers perform these complicated operations without genuine understanding. Evidently, comprehension additionally has to be based upon some biologically significant ground. It is hypothesized that the subjective feeling of understanding appears when current information is attributed to a definite need, which is scaled in sign (+/−) coordinates. This coordinate system ceases the brain calculations, when “comprehension” is reached, i.e., the acceptable level of need satisfaction is attained.
ER  - 

TY  - JOUR
T1  - Rule-based reinforcement learning methodology to inform evolutionary algorithms for constrained optimization of engineering applications
AU  - Radaideh, Majdi I.
AU  - Shirvan, Koroush
JO  - Knowledge-Based Systems
VL  - 217
SP  - 106836
PY  - 2021
DA  - 2021/04/06/
SN  - 0950-7051
DO  - https://doi.org/10.1016/j.knosys.2021.106836
UR  - https://www.sciencedirect.com/science/article/pii/S095070512100099X
KW  - Reinforcement learning
KW  - RL-guided evolutionary computation
KW  - proximal policy optimization
KW  - constrained combinatorial optimization
KW  - nuclear fuel assembly
AB  - For practical engineering optimization problems, the design space is typically narrow, given all the real-world constraints. Reinforcement Learning (RL) has commonly been guided by stochastic algorithms to tune hyperparameters and leverage exploration. Conversely in this work, we propose a rule-based RL methodology to guide evolutionary algorithms (EA) in constrained optimization. First, RL proximal policy optimization agents are trained to master matching some of the problem rules/constraints, then RL is used to inject experiences to guide various evolutionary/stochastic algorithms such as genetic algorithms, simulated annealing, particle swarm optimization, differential evolution, and natural evolution strategies. Accordingly, we develop RL-guided EAs, which are benchmarked against their standalone counterparts. RL-guided EA in continuous optimization demonstrates significant improvement over standalone EA for two engineering benchmarks. The main problem analyzed is nuclear fuel assembly combinatorial optimization with high-dimensional and computationally expensive physics. The results demonstrate the ability of RL to efficiently learn the rules that nuclear fuel engineers follow to realize candidate solutions. Without these rules, the design space is large for RL/EA to find many candidates. With imposing the rule-based RL methodology, we found that RL-guided EA outperforms standalone algorithms by a wide margin, with >10 times improvement in exploration capabilities and computational efficiency. These insights imply that when facing a constrained problem with numerous local optima, RL can be useful in focusing the search space in the areas where expert knowledge has demonstrated merit, while evolutionary/stochastic algorithms utilize their exploratory features to improve the number of feasible solutions.
ER  - 

TY  - CHAP
T1  - Material Design-for-eXcellence Framework – Application to Composites
AU  - Sousa, S.P.B.
AU  - Baptista, A.J.
AU  - Marques, A.T.
A2  - Brabazon, Dermot
BT  - Encyclopedia of Materials: Composites
PB  - Elsevier
CY  - Oxford
SP  - 290
EP  - 301
PY  - 2021
DA  - 2021/01/01/
SN  - 978-0-12-819731-8
DO  - https://doi.org/10.1016/B978-0-12-819724-0.00105-1
UR  - https://www.sciencedirect.com/science/article/pii/B9780128197240001051
KW  - Advanced materials
KW  - CFRP
KW  - Composites
KW  - Eco-efficiency
KW  - GFRP
KW  - Material Design-for-eXcellence
KW  - Materials Life Cycle
KW  - Materials Performance
KW  - M-DfX Scorecards
KW  - Sustainability
AB  - Despite a good number of material selection methods and tools, there is a lack of straightforward material performance methods that allow an easy and multi-dimensional assessment of the material properties, relating to the inner structure of the material in a multi-scale proposition, supported by a Life-Cycle Assessment mindset. Material Design-for-eXcellence (M-DfX) is a novel approach to support the assessment of material performance in a systematic and visual way, through the evaluation of material properties (“X” dimensions) and characteristics in a normalized form. It manages the material composition complexity in different scales, adopting a modular configuration analogy. The integrated analysis of Material Performance is attained via an effectiveness assessment of the properties’ characteristics, cross-evaluated with efficiency/eco-efficiency aspects within a Life Cycle approach, resulting in new original scorecards and quadrants graphical tools. It has adopted a Lean Thinking approach for the use of visual management and waste identification in relation to production and resource efficiency. A demonstration example of M-DfX is given for the framework testing in the Composite Materials field, comparing a CRFP composite versus GRFP composite real use case application for the body of an airport bus vehicle.
ER  - 

TY  - JOUR
T1  - Envisioning the future of ‘big data’ biomedicine
AU  - Bui, Alex A.T.
AU  - Van Horn, John Darrell
JO  - Journal of Biomedical Informatics
VL  - 69
SP  - 115
EP  - 117
PY  - 2017
DA  - 2017/05/01/
SN  - 1532-0464
DO  - https://doi.org/10.1016/j.jbi.2017.03.017
UR  - https://www.sciencedirect.com/science/article/pii/S1532046417300709
KW  - Biomedicine
KW  - Data science
KW  - Software
KW  - Computing
KW  - Training
AB  - Through the increasing availability of more efficient data collection procedures, biomedical scientists are now confronting ever larger sets of data, often finding themselves struggling to process and interpret what they have gathered. This, while still more data continues to accumulate. This torrent of biomedical information necessitates creative thinking about how the data are being generated, how they might be best managed, analyzed, and eventually how they can be transformed into further scientific understanding for improving patient care. Recognizing this as a major challenge, the National Institutes of Health (NIH) has spearheaded the “Big Data to Knowledge” (BD2K) program – the agency’s most ambitious biomedical informatics effort ever undertaken to date. In this commentary, we describe how the NIH has taken on “big data” science head-on, how a consortium of leading research centers are developing the means for handling large-scale data, and how such activities are being marshalled for the training of a new generation of biomedical data scientists. All in all, the NIH BD2K program seeks to position data science at the heart of 21st Century biomedical research.
ER  - 

TY  - JOUR
T1  - Empathic accuracy in coach–athlete dyads who participate in team and individual sports
AU  - Lorimer, Ross
AU  - Jowett, Sophia
JO  - Psychology of Sport and Exercise
VL  - 10
IS  - 1
SP  - 152
EP  - 158
PY  - 2009
DA  - 2009/01/01/
SN  - 1469-0292
DO  - https://doi.org/10.1016/j.psychsport.2008.06.004
UR  - https://www.sciencedirect.com/science/article/pii/S1469029208000526
KW  - Empathy
KW  - Understanding
KW  - Interaction
KW  - Coach–athlete dyads
AB  - Objective
The purpose of the present study was to investigate the empathic accuracy of coach–athlete dyads participating in team and individual sports.
Method
An adaptation of Ickes's [2001. Measuring empathic accuracy. In J. A. Hall & F. J. Bernieri (Eds.), Interpersonal sensitivity (pp. 219–242). Mahwah, NJ: Lawrence Erlbaum Associates] unstructured dyadic interaction paradigm was used to assess the empathic accuracy of 40 coach–athlete dyads. Accordingly, each dyad was filmed during a training session. The dyad members viewed selected video footage that displayed discrete interactions that had naturally occurred during that session. Dyad members reported what they remembered thinking/feeling while making inferences about what their partner's thought/felt at each point. Empathic accuracy was estimated by comparing self-reports and inferences.
Results
The results indicted that accuracy for coaches in individual sports was higher than coaches in team sports. Shared cognitive focus also differed between team and individual sports, and fully mediated the effect of sport-type on coach empathic accuracy. Moreover, coaches whose training sessions were longer demonstrated increased empathic accuracy. No differences were found for athletes.
Conclusions
The results suggest that the dynamics of the interaction between a coach and an athlete play a key role in how accurately they perceive each other.
ER  - 

TY  - JOUR
T1  - Creative, internally-directed cognition is associated with reduced BOLD variability
AU  - Roberts, Reece P.
AU  - Grady, Cheryl L.
AU  - Addis, Donna Rose
JO  - NeuroImage
VL  - 219
SP  - 116758
PY  - 2020
DA  - 2020/10/01/
SN  - 1053-8119
DO  - https://doi.org/10.1016/j.neuroimage.2020.116758
UR  - https://www.sciencedirect.com/science/article/pii/S1053811920302457
KW  - Episodic simulation
KW  - Imagination
KW  - Creativity
KW  - BOLD variability
AB  - In a range of externally-directed tasks, intra-individual variability of fMRI BOLD signal has been shown to be a stronger predictor of cognitive performance than mean BOLD signal. BOLD variability’s strong association with cognitive performance is hypothesised to be due to it capturing the dynamic range of neural systems. Although increased BOLD variability is also speculated to play a role in internally-directed thought, particularly when creative and flexible cognition is required, there is a relative lack of research exploring whether BOLD variability is related to internally-directed cognition. Thus, we investigated the relationship between BOLD variability and a key component of creativity – divergent thinking – in various tasks that required participants to think flexibly. We also determined whether any associations between BOLD variability and creativity overlapped with, or differed, from associations between mean BOLD signal and creativity. First, we performed task Partial Least Squares (PLS) analyses that compared BOLD signal (either mean or variability) during two future imagination conditions that differed in the amount of cognitive flexibility required: a Congruent condition in which autobiographical details (people, places, objects) comprising an imagined event belonged to the same social sphere (e.g., university) and an Incongruent condition in which details belonged to different social spheres and required greater cognitive flexibility to integrate. Results indicated that the Incongruent condition was associated with a widespread reduction in both BOLD variability and mean signal (relative to the Congruent condition), but in largely non-overlapping regions. Next, we used behavioral PLS to determine whether individual differences in performance on future simulation tasks as well as the Alternate Uses Task relates to BOLD variability and mean BOLD signal. Better performance on these tasks was predominantly associated with increases in mean BOLD signal and decreases in BOLD variability, in a range of disparate brain regions. Together, the results suggest that, unlike tasks requiring externally-directed cognition, superior performance on tasks requiring creative internal mentation is associated with less (not more) variability.
ER  - 

TY  - JOUR
T1  - Unpacking the essential tension of knowledge recombination: Analyzing the impact of knowledge spanning on citation impact and disruptive innovation
AU  - Wang, Cheng-Jun
AU  - Yan, Lihan
AU  - Cui, Haochuan
JO  - Journal of Informetrics
VL  - 17
IS  - 4
SP  - 101451
PY  - 2023
DA  - 2023/11/01/
SN  - 1751-1577
DO  - https://doi.org/10.1016/j.joi.2023.101451
UR  - https://www.sciencedirect.com/science/article/pii/S1751157723000767
KW  - Knowledge Recombination
KW  - Category spanning
KW  - Disruption
KW  - Citation
KW  - Team size
AB  - Drawing on the theories of knowledge recombination, we aim to unpack the essential tension between tradition and innovation in scientific research. Using the American Physical Society data and computational methods, we analyze the relationship between knowledge spanning, citation impact, and disruptive innovation. The findings show that there exists a U-shaped relationship between knowledge spanning and disruptive innovation. In contrast, there is an inverted U-shaped relationship between knowledge spanning and citation impact, and the inverted U-shaped effect is moderated by team size. This study contributes to the theories of knowledge recombination by suggesting that either intellectual conformism or knowledge recombination can lead to disruptive innovation. That is, when evaluating the quality of scientific research with disruptive innovation, the essential tension seems to disappear.
ER  - 

TY  - JOUR
T1  - Representing stuff in the human brain
AU  - Schmid, Alexandra C
AU  - Doerschner, Katja
JO  - Current Opinion in Behavioral Sciences
VL  - 30
SP  - 178
EP  - 185
PY  - 2019
DA  - 2019/12/01/
T2  - Visual perception
SN  - 2352-1546
DO  - https://doi.org/10.1016/j.cobeha.2019.10.007
UR  - https://www.sciencedirect.com/science/article/pii/S2352154619300816
AB  - Our experience of materials does not merely comprise judgments of single properties such as glossiness or roughness but is rather made up of a multitude of simultaneous impressions of qualities. To understand the neural mechanisms yielding such complex impressions, we suggest that it is necessary to extend existing experimental approaches to those that view material perception as a distributed and dynamic process. A distributed representations framework not only fits better with our perceptual experience of material qualities, it is commensurate with recent psychophysics and neuroimaging results.
ER  - 

TY  - JOUR
T1  - How can we improve process hazard identification? What can accident investigation methods contribute and what other recent developments? A brief historical survey and a sketch of how to advance
AU  - Pasman, Hans J.
AU  - Rogers, William J.
AU  - Mannan, M. Sam
JO  - Journal of Loss Prevention in the Process Industries
VL  - 55
SP  - 80
EP  - 106
PY  - 2018
DA  - 2018/09/01/
SN  - 0950-4230
DO  - https://doi.org/10.1016/j.jlp.2018.05.018
UR  - https://www.sciencedirect.com/science/article/pii/S0950423018300329
KW  - Accident-incident investigation
KW  - Hazard identification
KW  - Causation
KW  - System approach
AB  - Risk assessment is essential for various purposes such as facility siting, safeguarding, and licensing. Hazard identification (HAZID), which suffers greatly from incompleteness, is still the weakest link in risk assessment. Of course, this recognition is not new and many efforts have been spent to improve the situation, of which some have been rather successful. To find out what can go wrong, creative divergent thinking is required. Hazard identification should result in scenario definition. In that respect, applying the present tools as HAZOP and FMEA there is still a great emphasis on the material and equipment aspects. In contrast, underlying management and leadership failure in its many forms reflecting in organizational and human failure, due to complexity, attracts much less attention. Unlike in HAZID, in accident investigation the occurrence of an event with nasty consequences is no doubt a fact, so there must be one or more causes and the traces will lead to them. Over the years, methods for accident and incident investigation have gone through a significant evolution. From the early-on simplistic domino stone model and the human operator always at fault, via models of latent failure due to failing management involvement and via extensive root cause analysis (RCA) to a system approach. Hence, in accident investigation, management failure appearing in the many possible forms of human and organizational factors, obtained already 30 years ago with the RCA technique much attention, while it nowadays culminates in the socio-technical system approach. So, the question arises whether for improved HAZID we can learn from the accident investigation experience. In addition, safer design and advances from static risk assessment towards more accurate predictive operational dynamic risk assessment and management, will also be enabled by possibilities offered by big data and analytics. Digitization, automation and simulation, hence computerization, will be of great help in improving the identification of hazards and tracing the corresponding scenarios. The paper reviews the developmental history of both accident investigation and hazard identification methodology; incidentally it will identify commonality and differences. On the basis of the comparison and of recent advances in computerization, the paper will investigate to what extent beneficial modifications and additions can be made to obtain a higher degree of completeness in HAZID.
ER  - 

TY  - CHAP
T1  - Sensory Systems
AU  - Walsh, V.
BT  - Reference Module in Neuroscience and Biobehavioral Psychology
PB  - Elsevier
PY  - 2017
DA  - 2017/01/01/
SN  - 978-0-12-809324-5
DO  - https://doi.org/10.1016/B978-0-12-809324-5.06867-X
UR  - https://www.sciencedirect.com/science/article/pii/B978012809324506867X
KW  - Auditory system
KW  - Multisensory integration
KW  - Nerves
KW  - Somatosensation
KW  - Visual system
AB  - Sensory systems have an old school ring to them, a very old school ring to them. In the 15th century Benedetti was able to write, “By means of nerves, the pathways of the senses are distributed like the roots and fibers of a tree” (Alessandro Benedetti, 1497). This is still a good place to start because it gives one a feel for the 3D structure of our sensory apparatus, but the challenge of understanding the senses has, of course, gone well beyond structure (which is not to imply that all structural descriptions are complete or that we have joined all the dots of structure–function relationships), and any serious scholar needs to have a working knowledge of the development, physiology, psychophysics (physiology without the blood), genetics, pathology, and computational models of the senses.
ER  - 

TY  - JOUR
T1  - Mapping the landscape: A scoping review of 21st century skills literature in secondary education
AU  - Kain, Christina
AU  - Koschmieder, Corinna
AU  - Matischek-Jauk, Marlies
AU  - Bergner, Sabine
JO  - Teaching and Teacher Education
VL  - 151
SP  - 104739
PY  - 2024
DA  - 2024/12/01/
SN  - 0742-051X
DO  - https://doi.org/10.1016/j.tate.2024.104739
UR  - https://www.sciencedirect.com/science/article/pii/S0742051X24002713
KW  - 21 century skills
KW  - Secondary school
KW  - Scoping review
KW  - PRISMA flow chart
AB  - 21st century skills prepare students to adapt to a rapidly changing world, ensuring their capability of continuous learning and problem-solving. This review provides a systematic overview of how 21st century skills are addressed in research. It focuses on the context of secondary education and uses a PRISMA flow diagram to analyze 82 research articles. Results reveal that research on 21st century skills focuses on educational stakeholders’ opinions and attitudes, their potential effects and how they are implemented or assessed. The findings highlight a need for research to enhance the implementation of 21st century skills in secondary education.
ER  - 

TY  - CHAP
T1  - 8.10 - Optimization of Marine Renewable Energy Systems
AU  - Piggott, Matthew D.
AU  - Kramer, Stephan C.
AU  - Funke, Simon W.
AU  - Culley, David M.
AU  - Angeloudis, Athanasios
A2  - Letcher, Trevor M.
BT  - Comprehensive Renewable Energy (Second Edition)
PB  - Elsevier
CY  - Oxford
SP  - 176
EP  - 220
PY  - 2022
DA  - 2022/01/01/
SN  - 978-0-12-819734-9
DO  - https://doi.org/10.1016/B978-0-12-819727-1.00179-5
UR  - https://www.sciencedirect.com/science/article/pii/B9780128197271001795
KW  - Tidal stream
KW  - Tidal range
KW  - Optimization
KW  - Modelling
AB  - Optimizing marine renewable energy systems to maximize performance is key to their success. However, a range of physical, environmental, engineering, economic as well as computational challenges means that this is not straightforward. This article considers this topic, focusing on those systems whose performance is coupled to the hydrodynamics providing the resource; tidal power represents a clear example of this. In such cases system design must be optimal in relation to the resource׳s magnitude as well as its spatial and temporal variation, which are all dependent on the system׳s configuration and operation and so cannot be assumed to be known at the design stage. Designing based on the ambient resource could lead to under-performance. Coupling between the design and the resource has implications for the complexity of the optimization problem and potential hydrodynamical and environmental impacts. This coupling distinguishes many marine energy systems from other renewables which do not impact in any significant manner on the resource. The optimal design of marine energy systems thus represents a challenging and somewhat unique problem. However, feedback also opens up a number of possibilities where the resource can be ‘controlled’, to maximize the cumulative power obtained from multiple devices or plants, or to achieve some other complementary goal. Design optimization is thus critical, with many issues to consider. Due to the complexity of the problem a computational based solution is a necessity in all but the simplest scenarios. However, the coupled feedback requires that an iterative solution approach be used, which combined while the vast range of spatial and temporal scales means that methodological compromises need to be made. These compromises need to be understood, with the correct computational tool used at the appropriate point in the design process. This article reviews these challenges as well as the progress that has been made in addressing them.
ER  - 

TY  - JOUR
T1  - The validity of an artificial intelligence application for assessment of orthodontic treatment need from clinical images
AU  - Talaat, Sameh
AU  - Kaboudan, Ahmed
AU  - Talaat, Wael
AU  - Kusnoto, Budi
AU  - Sanchez, Flavio
AU  - Elnagar, Mohammed H.
AU  - Bourauel, Christoph
AU  - Ghoneima, Ahmed
JO  - Seminars in Orthodontics
VL  - 27
IS  - 2
SP  - 164
EP  - 171
PY  - 2021
DA  - 2021/06/01/
T2  - Artificial Intelligence applications in Orthodontics -An update
SN  - 1073-8746
DO  - https://doi.org/10.1053/j.sodo.2021.05.012
UR  - https://www.sciencedirect.com/science/article/pii/S1073874621000359
AB  - Aim: To assess the validity of a Convolutional Neural Network (CNN) digital model to detect and localize orthodontic malocclusions from intraoral clinical images. Materials and methods: The sample of this study consisted of the intraoral images of 700 Subjects. All images were intraoral clinical images, in one of the following views: Left Occlusion, Right Occlusion, Front Occlusion, Upper Occlusal, and Lower Occlusal. The following malocclusion conditions were localized: crowding, spacing, increased overjet, cross bite, open bite, deep bite. The images annotations were repeated by the same investigator (S.T) with a one week interval (ICC ≥ 0.9). The CNN model used for this research study was the “You Only Look Once” model. This model can detect and localize multiple objects or multiple instances of the same object in each image. It is a fully convolutional deep neural network; 24 convolutional layers followed by 2 fully connected layers. This model was implemented using the TensorFlow framework freely available from Google. Results: The created CNN model was able to detect and localize the malocclusions with an accuracy of 99.99%, precision of 99.79%, and a recall of 100%. Conclusions: The use of computational deep convolutional neural networks to identify and localize orthodontic problems from clinical images proved valid. The built AI engine accurately detected and localized malocclusion from different views of intra-oral clinical images.
ER  - 

TY  - JOUR
T1  - Understanding the cell: Future views of structural biology
AU  - Beck, Martin
AU  - Covino, Roberto
AU  - Hänelt, Inga
AU  - Müller-McNicoll, Michaela
JO  - Cell
VL  - 187
IS  - 3
SP  - 545
EP  - 562
PY  - 2024
DA  - 2024/02/01/
SN  - 0092-8674
DO  - https://doi.org/10.1016/j.cell.2023.12.017
UR  - https://www.sciencedirect.com/science/article/pii/S0092867423013491
KW  - structural biology
KW  - digital twin
KW  - computational modeling
KW  - cellular self-organization
AB  - Summary
Determining the structure and mechanisms of all individual functional modules of cells at high molecular detail has often been seen as equal to understanding how cells work. Recent technical advances have led to a flush of high-resolution structures of various macromolecular machines, but despite this wealth of detailed information, our understanding of cellular function remains incomplete. Here, we discuss present-day limitations of structural biology and highlight novel technologies that may enable us to analyze molecular functions directly inside cells. We predict that the progression toward structural cell biology will involve a shift toward conceptualizing a 4D virtual reality of cells using digital twins. These will capture cellular segments in a highly enriched molecular detail, include dynamic changes, and facilitate simulations of molecular processes, leading to novel and experimentally testable predictions. Transferring biological questions into algorithms that learn from the existing wealth of data and explore novel solutions may ultimately unveil how cells work.
ER  - 

TY  - JOUR
T1  - Efficient algorithms for large scale linear system identification using stable spline estimators
AU  - Carli, Francesca P.
AU  - Chiuso, Alessandro
AU  - Pillonetto, Gianluigi
JO  - IFAC Proceedings Volumes
VL  - 45
IS  - 16
SP  - 119
EP  - 124
PY  - 2012
DA  - 2012/07/01/
T2  - 16th IFAC Symposium on System Identification
SN  - 1474-6670
DO  - https://doi.org/10.3182/20120711-3-BE-2027.00394
UR  - https://www.sciencedirect.com/science/article/pii/S1474667015379386
KW  - Parametric prediction error methods
KW  - output error models
KW  - model complexity
KW  - marginal likelihood
KW  - kernel eigenfunctions
AB  - A new nonparametric approach for system identification has been recently proposed where, in place of postulating parametric classes of impulse responses, the estimation process starts from an infinite-dimensional space. In particular, the impulse response is seen as the realization of a zero-mean Gaussian process. Its covariance, the so called stable spline kernel, encodes information on system stability and depends on few hyperparameters estimated from data via marginal likelihood optimization. This approach has been proved to compare much favorably with classical parametric methods but, in data rich situations, a possible drawback may be represented by its computational complexity which scales with the cube of the number of available samples. In this work we design a new computational strategy which may reduce significantly the computational load required by the stable spline estimator, thus extending its practical applicability also to large-scale scenarios.
ER  - 

TY  - JOUR
T1  - Using neural networks as models of personality process: A tutorial
AU  - Read, Stephen J.
AU  - Droutman, Vita
AU  - Smith, Benjamin J.
AU  - Miller, Lynn C.
JO  - Personality and Individual Differences
VL  - 136
SP  - 52
EP  - 67
PY  - 2019
DA  - 2019/01/01/
T2  - Dynamic Personality Psychology
SN  - 0191-8869
DO  - https://doi.org/10.1016/j.paid.2017.11.015
UR  - https://www.sciencedirect.com/science/article/pii/S0191886917306724
KW  - Neural networks
KW  - Computational modeling
KW  - Within-subjects variability
KW  - Connectionist modeling
KW  - Personality dynamics
AB  - This paper presents a tutorial for creating neural network models of personality processes. Such models enable researchers to create explicit models of both personality structure and personality dynamics, and to address issues of recent concern in personality, such as, “If personality is stable, then how is it possible that within subject variability in personality states can be as large as or larger than between subject variability in personality?” or “Is it possible to understand personality dynamics and personality structure within a common framework?” We discuss why one should want to use neural networks, review what a neural network model is, review a previous model we have constructed, discuss how to conceptualize issues in such a way that they can be computationally modeled, show how that conceptualization can be translated into a model, and discuss the utility of such models for understanding personality structure and personality dynamics. To build our model we use a neural network modeling package called emergent that is freely available, and a specific architecture called Leabra to build a runnable model that addresses one of the questions posed above: How can within subject variability in personality related states be as large as between subject variability in personality?
ER  - 

TY  - JOUR
T1  - APRS: Automatic pruning ratio search using Siamese network with layer-level rewardsImage 1
AU  - Xiao, Huachao
AU  - Wang, Yangxin
AU  - Liu, Jianyi
AU  - Huo, Jiaxin
AU  - Hu, Yang
AU  - Wang, Yu
JO  - Digital Signal Processing
VL  - 133
SP  - 103864
PY  - 2023
DA  - 2023/03/01/
SN  - 1051-2004
DO  - https://doi.org/10.1016/j.dsp.2022.103864
UR  - https://www.sciencedirect.com/science/article/pii/S105120042200481X
KW  - Structured pruning
KW  - Deep reinforcement learning
KW  - Pruning ratio search
KW  - Siamese network
AB  - Structured pruning is still a mainstream model compression technique, for its merit of easy to implement and no reliance on specific hardware supporting library. In most previous works, the layer-wise channel pruning ratios were determined empirically. In this paper, we propose an Automatic Pruning Ratio Search (APRS) algorithm that can find the layer-wise optimal pruning ratio within the deep reinforcement learning framework. To solve the coarse-granularity reward problem existing in some previous works like AMC and CACP, a novel layer-level reward function is designed based on the Siamese network architecture for the fine-granularity agent-environment interaction purpose. We use a computationally efficient way to evaluate the effect of pruning action on each single layer. The incurred “backwardness disadvantage” problem has also been analyzed and addressed. The experiments are performed using the VGG-16, and MobileNet-v1 on the CIFAR10/100 and UC Merced Land-use datasets. The results verified that our method can better reveal the underlying sparse sensitivities of different layers in both high redundancy networks and compact networks, so that resulting a higher network accuracy after pruning compared to the traditional methods.
ER  - 

TY  - JOUR
T1  - Model validation hierarchies for connecting system design to modeling and simulation capabilities
AU  - Luckring, James M.
AU  - Shaw, Scott
AU  - Oberkampf, William L.
AU  - Graves, Rick E.
JO  - Progress in Aerospace Sciences
VL  - 142
SP  - 100950
PY  - 2023
DA  - 2023/10/01/
SN  - 0376-0421
DO  - https://doi.org/10.1016/j.paerosci.2023.100950
UR  - https://www.sciencedirect.com/science/article/pii/S0376042123000660
KW  - Modeling and simulation
KW  - Validation hierarchy
KW  - Systems architecture
KW  - Physics taxonomy
KW  - Surface-to-air missile defense system
AB  - Hierarchical structures provide a means to systematically deconstruct an engineering system of arbitrary complexity into its subsystems, components, and physical processes. Model validation hierarchies can aid in understanding the coupling and interaction of subsystems and components, as well as improve the understanding of how simulation models are used to design and optimize the engineering system of interest. The upper tiers of the hierarchy address systems and subsystems architecture decompositions, while the lower tiers address physical processes that are both coupled and uncoupled. Recent work connects these two general sections of the hierarchy through a transition tier, which blends the focus of system functionality and physics modeling activities. This work also includes a general methodology for how a model validation hierarchy can be constructed for any type of engineering system in any operating environment, e.g., land, air, sea, or space. We review previous work on the construction and use of model validation hierarchies in not only the field of aerospace systems, but also from commercial nuclear power plant systems. Then an example of a detailed model validation hierarchy is constructed and discussed for a surface-to-air missile defense system with an emphasis on the missile subsystems.
ER  - 

TY  - JOUR
T1  - MPEG-4 AVC stream-based saliency detection. Application to robust watermarking
AU  - Ammar, Marwa
AU  - Mitrea, Mihai
AU  - Hasnaoui, Marwen
AU  - Le Callet, Patrick
JO  - Signal Processing: Image Communication
VL  - 60
SP  - 116
EP  - 130
PY  - 2018
DA  - 2018/02/01/
SN  - 0923-5965
DO  - https://doi.org/10.1016/j.image.2017.09.007
UR  - https://www.sciencedirect.com/science/article/pii/S0923596517301674
KW  - Saliency map
KW  - MPEG-4 AVC stream
KW  - Density fixation map
KW  - Saccade locations
KW  - Robust watermarking
AB  - By bridging uncompressed-domain saliency detection and MPEG-4 AVC compression principles, the present paper advances a methodological framework for extracting the saliency maps directly from the stream syntax elements. In this respect, inside each GOP, the intensity, color, orientation and motion elementary saliency maps are related to the energy of the luma coefficients, to the energy of chroma coefficients, to the gradient of the prediction modes and to the amplitude of the motion vectors, respectively. The three spatial saliency maps are pooled according to an average formula, while the static-temporal fusion is achieved by six different formulas. The experiments consider both ground-truth and applicative evaluations. The ground-truth benchmarking investigates the relation between the predicted MPEG-4 AVC saliency map and the actual human saliency, captured by eye-tracking devices. It is based on two corpora (representing density fixation maps and saccade locations), two objective criteria (related to the closeness between the predicted and the real saliency maps and to the difference between the behavior of the predicted saliency map in fixation and random locations), two objective measures (KLD – the Kullback Leibler Divergence and AUC – the Area Under the ROC Curve) and 5 state-of-the-art saliency models (3 acting in spatial domain and 2 acting in compressed domain). The applicative validation is carried out by integrating the MPEG-4 AVC saliency map into a robust watermarking application. As an overall conclusion, the paper demonstrates that although the MPEG-4 AVC standard does not explicitly relies on any visual saliency principle, its stream syntax elements preserve this property. Four main benefits for the MPEG-4 AVC based saliency extraction are thus brought to light: (1) it outperforms (or, at least, is as good as) state-of-the-art uncompressed domain methods, (2) it allows significant gains to be obtained in watermarking transparency (for prescribed data payload and robustness), (3) it is less sensitive to the randomness in the processed visual content, and (4) it has a linear computational complexity. For instance, the ground truth results exhibit absolute relative gains between 60% and 164% in KLD, between 17% and 21% in AUC, and relative gains in KLD sensitivity between 1.18 and 6.12 and in AUC sensitivity between 1.06 and 33.7; the applicative validation brings to light transparency gains up to 10 dB in PSNR.
ER  - 

TY  - JOUR
T1  - Capturing distinctions while mining text data: Toward low-tech formalization for text analysis
AU  - Breiger, Ronald L.
AU  - Wagner-Pacifici, Robin
AU  - Mohr, John W.
JO  - Poetics
VL  - 68
SP  - 104
EP  - 119
PY  - 2018
DA  - 2018/06/01/
SN  - 0304-422X
DO  - https://doi.org/10.1016/j.poetic.2018.02.005
UR  - https://www.sciencedirect.com/science/article/pii/S0304422X17301584
KW  - Text mining
KW  - Hermeneutics
KW  - National security
KW  - Computational sociology
KW  - Big data
KW  - Close reading
AB  - In this article we consider some low-tech approaches to text mining. Our goal is to articulate a RiCH (Reader in Control of Hermeneutics) style of text analysis that takes advantage of the digital affordances of modern reading practices and easily deployable computational tools while also preserving the primacy of the interpretive lens of the human reader. In the article we offer three analytical interventions that are suitable to the low-tech formalizations we propose: the first and most developed intervention tracks the (normally computationally ignored) “stop” words; the second identifies the use of strategic anxiety terms in the texts; and the third (less developed in this article) introduces the grammatical features of modality (including modalization statements of probability and usuality, and modulation statements regarding degrees of obligation and inclination). All three analytical interventions provide a productive tracking of various modes and degrees of strategic decisiveness, contradiction, uncertainty and indeterminacy in a corpus of recent U.S. National Security Strategy reports.
ER  - 

TY  - JOUR
T1  - Symmetry exploitation to reduce impedance evaluations in grounding grids
AU  - Vieira, Pedro H.N.
AU  - Moura, Rodolfo A.R.
AU  - Schroeder, Marco Aurélio O.
AU  - Lima, Antonio C.S.
JO  - International Journal of Electrical Power & Energy Systems
VL  - 123
SP  - 106268
PY  - 2020
DA  - 2020/12/01/
SN  - 0142-0615
DO  - https://doi.org/10.1016/j.ijepes.2020.106268
UR  - https://www.sciencedirect.com/science/article/pii/S0142061519342188
KW  - Electromagnetic analysis
KW  - Frequency response
KW  - Grounding
KW  - Method of moments
KW  - Numerical methods
KW  - Symmetry
AB  - One main concern on wideband evaluation of grounding systems is the high computational burden related to the determination of the impedance matrices. Traditionally, one has to divide any given conductor in a large number of segments which leads to a rather time consuming procedure. However, there are a number of geometrical symmetries that if exploited can significantly reduce the overall computational time. This work aims at investigating the adequacy of using some existing symmetries to reduce computer burden in the assessment of a wideband grounding system in models based on the Method of Moments. An algorithmic approach is proposed to extend the symmetry exploitation to arbitrarily oriented uniform rectangular grounding systems. Several topologies are used to assess the performance of the proposed approach. According to results, the proposed methodology can be more than 12 times faster than the traditional approach without loss of accuracy because it is not a numerical approximation.
ER  - 

TY  - JOUR
T1  - An optimal Best-Worst prioritization method under a 2-tuple linguistic environment in decision making
AU  - Labella, Álvaro
AU  - Dutta, Bapi
AU  - Martínez, Luis
JO  - Computers & Industrial Engineering
VL  - 155
SP  - 107141
PY  - 2021
DA  - 2021/05/01/
SN  - 0360-8352
DO  - https://doi.org/10.1016/j.cie.2021.107141
UR  - https://www.sciencedirect.com/science/article/pii/S0360835221000450
KW  - Best-Worst method
KW  - 2-tuple linguistic model
KW  - Multi-criteria group decision making
AB  - Multi-criteria group decision making (MCGDM) deals with decision makers who evaluate alternatives over several criteria. MCGDM problems evolve in tandem with the progress of our society. Such progress has given rise to the large-scale group decision making (LS-GDM) problems in which hundreds of decision makers may participate in the decision process and new challenges to face such as groups’ formation and polarization opinions. Most real world MCGDM problems present changing contexts with uncertainty that cannot be modeled by numerical values. Under these circumstances, the use of linguistic variables and computing with words (CW) processes have provided successfully results. Concretely, the 2-tuple linguistic computational model stands out because its precise linguistic computations and high interpretability. On the other hand, pairwise comparison is a widely used elicitation technique in MCGDM, but a large number of comparisons might lead inconsistent decision makers’ preferences. The Best-Worst method (BWM) reduces the number of pairwise comparisons and the inconsistency in decision makers’ opinions. Several BWM approaches have been proposed to manage linguistic information but none of them take advantage of the 2-tuple linguistic computational process based on the CW approach, which would allow to obtain precise and understandable results. This paper aims to present an extended 2-tuple BWM to reduce the number of pairwise comparisons in MCGDM problems and model the uncertainty associated with them to accomplish accuracy computations and obtaining interpretable results. Moreover, we apply our proposal to LS-GDM scenarios in which polarization opinions and sub-groups identification, ignored from any of BWM proposals, are considered. Finally, the new model is applied to several illustrative MCGDM problems.
ER  - 

TY  - JOUR
T1  - Closing the gap: Merging engineering and anthropology in holistic fire safety assessments in the maritime and offshore industries
AU  - Karsten, Mette Marie Vad
AU  - Ruge, Aqqalu Thorbjørn
AU  - Hulin, Thomas
JO  - Safety Science
VL  - 122
SP  - 104512
PY  - 2020
DA  - 2020/02/01/
SN  - 0925-7535
DO  - https://doi.org/10.1016/j.ssci.2019.104512
UR  - https://www.sciencedirect.com/science/article/pii/S0925753518316175
KW  - Anthropology
KW  - Fire safety engineering
KW  - Transdisciplinary
KW  - Risk
KW  - Maritime
KW  - Offshore
AB  - This article reports on the endeavor to merge the fields of anthropology and fire safety engineering in holistic fire safety assessments within the maritime and offshore industries. The article suggests a combination of the two disciplines to transition from an interdisciplinary approach towards transdisciplinarity. The approach has been developed and adjusted during three cases of risk analyses and prevention strategies on fire safety. The article presents two methodological insights illustrating the necessary attitude of interdisciplinarity as a foundation towards transdisciplinarity. It advocates for the need of willingness in organizations and project teams to consider both disciplines as equally valid, integrate them in research definition, and create a base for common understanding. Subsequently, it is proposed that transdisciplinary work requires the creation of a group of core members acting as guarantors of transdisciplinarity, thus becoming themselves transdisciplinary humans working in a joined framework of thinking and methods. The article also presents two operational findings integrating the two disciplines within the area of fire safety. The first finding concerns including ‘daily operations’ in fire safety design, as daily practices and perceptions among crew can have a high impact on fire safety. The second finding concerns ‘reclassification of space and place’. It highlights mixing and shifting between work- and leisure-related practices within the same physical space, leading to the identification of new fire scenarios. It also explores the shifts between work, leisure, and emergency places, and their link to the shifts in professional roles of crew.
ER  - 

TY  - CHAP
T1  - Chapter 27 The Computation of Prices Indices
AU  - Ginsburgh, Victor
AU  - Mei, Jianping
AU  - Moses, Michael
A2  - Ginsburg, Victor A.
A2  - Throsby, David
BT  - Handbook of the Economics of Art and Culture
PB  - Elsevier
VL  - 1
SP  - 947
EP  - 979
PY  - 2006
DA  - 2006/01/01/
SN  - 1574-0676
DO  - https://doi.org/10.1016/S1574-0676(06)01027-1
UR  - https://www.sciencedirect.com/science/article/pii/S1574067606010271
KW  - prices indices
KW  - repeat sales
KW  - hedonic pricing
KW  - auctions
AB  - While there are no significant investment characteristics that inhibit art from being considered as an asset, a major hurdle has long been the lack of a systematic measure of its financial performance. Due to its heterogeneity (each piece is different) and its infrequency of trading (the exact same piece does not come to the market very often), the determination of changes in market value is difficult to ascertain. Two estimation methods are commonly used to construct indices. Repeat-sales regression (RSR) uses prices of individual objects traded at two distinct moments in time. If the characteristics of an object do not change (which is usually so for collectibles), the heterogeneity issue is bypassed. The basic idea of the hedonic regression (HR) method is to regress prices on various attributes of objects (dimensions, artist, subject matter, etc.) and to use the residuals of the regression which can be considered as “characteristic-free prices” to compute the price index. The chapter deals with the basics of hedonic and repeat-sales estimators, and tries to interpret in economic terms what both are trying to achieve. It also goes into some more technical details which may be useful for researchers who want to construct such indices, and gives some guidelines on how to go about collecting data, and the choice between RSR and HR that this induces. Both methods are compared using simulated returns, pointing to which method should be used given the data at hand.
ER  - 

TY  - JOUR
T1  - Fuzzy Cognitive Maps for futures studies—A methodological assessment of concepts and methods
AU  - Jetter, Antonie J.
AU  - Kok, Kasper
JO  - Futures
VL  - 61
SP  - 45
EP  - 57
PY  - 2014
DA  - 2014/09/01/
SN  - 0016-3287
DO  - https://doi.org/10.1016/j.futures.2014.05.002
UR  - https://www.sciencedirect.com/science/article/pii/S0016328714000809
KW  - Fuzzy Cognitive Maps
KW  - Future studies
KW  - Scenarios
KW  - Mental model
KW  - System thinking
AB  - Fuzzy Cognitive Map (FCM) modelling is highly suitable for the demands of future studies: it uses a mix of qualitative and quantitative approaches, it enables the inclusion of multiple and diverse sources to overcome the limitations of expert opinions, it considers multivariate interactions that lead to nonlinearities, and it aims to make implicit assumptions (or mental models) explicit. Despite these properties, the field of future studies is slow to adopt FCM and to apply the increasingly solid theoretical foundations and rigorous practices for FCM applications that are evolving in other fields. This paper therefore discusses theoretical and practical aspects of constructing and applying FCMs within the context of future studies: based on an extensive literature review and the authors’ experience with FCM projects, it provides an introduction of fundamental concepts of FCM modelling, a step-wise description and discussion of practical methods and their pitfalls, and an overview over future research directions for FCM in future studies.
ER  - 

TY  - JOUR
T1  - Application domain extension of incremental capacity-based battery SoH indicators
AU  - Ospina Agudelo, Brian
AU  - Zamboni, Walter
AU  - Monmasson, Eric
JO  - Energy
VL  - 234
SP  - 121224
PY  - 2021
DA  - 2021/11/01/
SN  - 0360-5442
DO  - https://doi.org/10.1016/j.energy.2021.121224
UR  - https://www.sciencedirect.com/science/article/pii/S0360544221014729
KW  - Battery
KW  - State of health
KW  - Battery ageing
KW  - Capacity degradation
KW  - Incremental capacity
KW  - Randomised usage pattern
AB  - The Incremental Capacity (IC) analysis is used to characterise the capacity and the battery state of health, aged by cycling patterns with randomly selected pulsed current levels and duration. The batteries are periodically characterised at 1C current, which is a high value with respect to the typical IC tests in pseudo-equilibrium condition. The high-current IC curves generation from raw voltage/current data includes two filtering stages, one for the input voltage and one for the incremental capacity curve smoothing, which are optimised for the application on the basis of the data characteristics. The correlations between the IC main peak features and the battery full capacity for 28 Lithium–Cobalt oxide batteries with 18650 packaging were evaluated, finding that the main peak area is a general feature to evaluate the state of health under high current tests and random usage pattern, and, therefore, it can be used as a battery health indicator in practical applications. The effects of the computational parameters on the relationship between the peak area and the battery capacity are also investigated. The results are confirmed by a further analysis performed over an additional set of cells with different technology, aged with a fixed cycling pattern. Additionally, the performance of the peak area as a health indicator was compared with an ohmic resistance-based estimation approach.
ER  - 

TY  - JOUR
T1  - Kandinsky Patterns
AU  - Müller, Heimo
AU  - Holzinger, Andreas
JO  - Artificial Intelligence
VL  - 300
SP  - 103546
PY  - 2021
DA  - 2021/11/01/
SN  - 0004-3702
DO  - https://doi.org/10.1016/j.artint.2021.103546
UR  - https://www.sciencedirect.com/science/article/pii/S0004370221000977
KW  - Explainable AI
KW  - Explainability
KW  - Synthetic test data
KW  - Ground truth
AB  - Kandinsky Figures and Kandinsky Patterns are mathematically describable, simple, self-contained hence controllable synthetic test data sets for the development, validation and training of visual tasks and explainability in artificial intelligence (AI). Whilst Kandinsky Patterns have these computationally manageable properties, they are at the same time easily distinguishable by human observers. Consequently, controlled patterns can be described by both humans and computers. We define a Kandinsky Pattern as a set of Kandinsky Figures, where for each figure an “infallible authority” defines that the figure belongs to the Kandinsky Pattern. With this simple principle we build training and validation data sets for testing explainability, interpretability and context learning. In this paper we describe the basic idea and some underlying principles of Kandinsky Patterns. We provide a Github repository and invite the international AI research community to a challenge to experiment with our Kandinsky Patterns. The goal is to help expand and advance the field of AI, and in particular to contribute to the increasingly important field of explainable AI.
ER  - 

TY  - CHAP
T1  - Chapter 7 - The social shaping of smart cities
AU  - Mora, Luca
AU  - Deakin, Mark
A2  - Mora, Luca
A2  - Deakin, Mark
BT  - Untangling Smart Cities
PB  - Elsevier
SP  - 215
EP  - 234
PY  - 2019
DA  - 2019/01/01/
SN  - 978-0-12-815477-9
DO  - https://doi.org/10.1016/B978-0-12-815477-9.00007-4
UR  - https://www.sciencedirect.com/science/article/pii/B9780128154779000074
KW  - Interdisciplinarity
KW  - Expectations
KW  - Social shaping
KW  - Open innovation
KW  - Technological advancement
KW  - Urban innovations
KW  - Technological change
KW  - Hype
KW  - Smart city
KW  - Smart city research
KW  - Lessons
KW  - Recommendations
KW  - Open community
KW  - Co-design
KW  - Collaboration
KW  - Collaborative environment
KW  - Quadruple helix
KW  - Triple helix
KW  - Organization dynamics
KW  - Urban innovation
KW  - Urban sustainability
KW  - Sustainable urban development
KW  - Hyped behaviors
KW  - Urban utopia
KW  - Expectations
KW  - Smart city development
KW  - Smart urbanism
AB  - This last chapter concludes the investigation by summing up the key lessons and recommendations that this book can offer to the community of stakeholders involved in smart city research, policy, and practice. The series of complementary analyses that the previous chapters report on demonstrate that, when untangled from the technocentric urban utopia pictured by the corporate sector, smart cities have the potential to develop into innovation systems that set the stage for a technology-enabled approach to urban sustainability. But realizing this opportunity requires to move beyond traditional boundaries, separate the hype from reality, and strengthen the focus on the social shaping of smart cities. The investigation demonstrates that, in order for such a social shaping to develop, the design of smart cities needs to be understood as a collective action in which two complementary forces are combined. On the one hand, the faith in the technological advancement exposed in the utopian thinking. On the other, the knowledge, skills, and interests of a quadruple-helix collaborative environment where the need for technological innovation in response to urban sustainability goals is not shaped by the corporate sector and its technocentric and market-oriented logic, but an open community whose actions serve the public interest and are based on a holistic interpretation of smart city development.
ER  - 

TY  - CHAP
T1  - Some Thermodynamics and Electrostatics With a View to Electrochemistry
AU  - Schiller, R.
A2  - Wandelt, Klaus
BT  - Encyclopedia of Interfacial Chemistry
PB  - Elsevier
CY  - Oxford
SP  - 246
EP  - 257
PY  - 2018
DA  - 2018/01/01/
SN  - 978-0-12-809894-3
DO  - https://doi.org/10.1016/B978-0-12-409547-2.13605-4
UR  - https://www.sciencedirect.com/science/article/pii/B9780124095472136054
KW  - Activity coefficient
KW  - Chemical potential
KW  - Cole–Cole plot
KW  - Conservation laws
KW  - Dielectric relaxation time
KW  - Dipole moment
KW  - Electric dipole
KW  - Entropy of mixing
KW  - Gauss law of electrostatics
KW  - Kramers–Kronig relations
KW  - Osmotic coefficient
KW  - Poisson equation
KW  - Polarization
KW  - Relative permittivity
KW  - Solvation energy
AB  - This article tries to offer an overview of some basic laws of thermodynamics and electrostatics which are considered to be part of the foundations of electrochemical thinking. Equilibrium thermodynamics is introduced in terms of conservation laws paying particular attention to the notion of chemical potential. After discussing the forces, potentials, and energetics of charges in vacuum the same problems are dealt with in continuous dielectric media. Here polarization, formation and role of dipole moments, their relation to relative permittivity (dielectric constant) are discussed both in macroscopic and atomic/molecular terms. Finally the kinetics of the response of relative permittivity to the variation of polarizing fields and charges are described. Solvation processes are referred to in connection with both thermodynamic and electrostatic considerations.
ER  - 

TY  - JOUR
T1  - Foundations of Programmable Process Structures for the unified modeling and simulation of agricultural and aquacultural systems
AU  - Varga, Monika
AU  - Csukas, Bela
JO  - Information Processing in Agriculture
VL  - 11
IS  - 1
SP  - 91
EP  - 108
PY  - 2024
DA  - 2024/03/01/
SN  - 2214-3173
DO  - https://doi.org/10.1016/j.inpa.2022.10.001
UR  - https://www.sciencedirect.com/science/article/pii/S2214317322000737
KW  - Unified process model
KW  - Meta-prototype-based architecture
KW  - Transition-based structure representation
KW  - Locally programmable functionality prototypes
KW  - Agricultural systems
KW  - Aquacultural systems
AB  - This research paper defines the theoretical foundations and computational implementation of a non-conventional modeling and simulation methodology, inspired by the needs of problem solving for biological, agricultural, aquacultural and environmental systems. The challenging practical problem is to develop a framework for automatic generation of causally right and balance-based, unified models that can also be applied for the effective coupling amongst the various (sophisticated field-specific, sensor data processing-based, upper level optimization-driven, etc.) models. The scientific problem addressed in this innovation is to develop Programmable Process Structures (PPS) by combining functional basis of systems theory, structural approach of net theory and computational principles of agent based modeling. PPS offers a novel framework for the automatic generation of easily extensible and connectible, unified models for the underlying complex systems. PPS models can be generated from one state and one transition meta-prototypes and from the transition oriented description of process structure. The models consist of unified state and transition elements. The local program containing prototype elements, derived also from the meta-prototypes, are responsible for the case-specific calculations. The integrity and consistency of PPS architecture are based on the meta-prototypes, prepared to distinguish between the conservation-laws-based measures and the signals. The simulation is based on data flows amongst the state and transition elements, as well as on the unification based data transfer between these elements and their calculating prototypes. This architecture and its AI language-based (Prolog) implementation support the integration of various field- and task-specific models, conveniently. The better understanding is helped by a simple example. The capabilities of the recently consolidated general methodology are discussed on the basis of some preliminary applications, focusing on the recently studied agricultural and aquacultural cases.
ER  - 

TY  - JOUR
T1  - Automatic detection of Alzheimer’s disease from EEG signals using low-complexity orthogonal wavelet filter banks
AU  - Puri, Digambar V.
AU  - Nalbalwar, Sanjay L.
AU  - Nandgaonkar, Anil B.
AU  - Gawande, Jayanand P.
AU  - Wagh, Abhay
JO  - Biomedical Signal Processing and Control
VL  - 81
SP  - 104439
PY  - 2023
DA  - 2023/03/01/
SN  - 1746-8094
DO  - https://doi.org/10.1016/j.bspc.2022.104439
UR  - https://www.sciencedirect.com/science/article/pii/S174680942200893X
KW  - Alzheimer’s disease
KW  - Electroencephalogram
KW  - Fractal dimension
KW  - Orthogonal filter banks
KW  - Support vector machine
KW  - Wavelets
AB  - Background:
Alzheimer’s disease (AD) is one of the most common neurodegenerative disorder. As the incidence of AD is rapidly increasing worldwide, detecting it at an early stage can prevent memory loss and cognitive dysfunctions in patients. Recently, Electroencephalogram (EEG) signals in AD cases show less synchronization and a slowing effect. The abrupt and transient behavior of EEG signals can be detected from specific frequency bands that are cortical rhythms of interest such as delta (0−4Hz), theta (4−8Hz), alpha (8−12Hz), beta1 (12−16Hz), beta2 (16−32Hz), and gamma (32−48Hz).
Method:
This paper proposes novel low-complexity orthogonal wavelet filter banks with vanishing moments (LCOWFBs-v) to decompose the AD and normal controlled (NC) EEG signals into subbands (SBs). A generalized design technique is suggested to reduce the computational complexity of original irrational wavelet filter banks (FBs). The two features, Higuchi’s fractal dimension (HFD) and Katz’s fractal dimension (KFD), were extracted from EEG SBs. The significance of these extracted features has been inspected using Kruskal–Wallis test.
Results:
The present study analyzed the EEG recordings of 23 subjects (AD-12 and NC-11) with the combination of LCOWFBs, HFD, and KFD. The proposed technique achieved a classification accuracy of 98.5% and 98.6% using the LCOWFBs-4 and LCOWFBs-6, respectively with a cubic-support vector machine classifier and 10-fold cross-validation technique.
Conclusion:
The proposed method with newly designed LCOWFBs is efficient compared with the well-known FBs and existing techniques for detecting AD.
ER  - 

TY  - JOUR
T1  - A programming grammar for robotic fabrication: Incorporating material agency into clay textures
AU  - Tokac, Iremnur
AU  - Bruyninckx, Herman
AU  - Moere, Andrew Vande
JO  - Design Studies
VL  - 88
SP  - 101220
PY  - 2023
DA  - 2023/09/01/
SN  - 0142-694X
DO  - https://doi.org/10.1016/j.destud.2023.101220
UR  - https://www.sciencedirect.com/science/article/pii/S0142694X23000613
KW  - human–computer interaction
KW  - computational model(s)
KW  - design technology
KW  - reflective practice
KW  - making grammars
AB  - Material agency describes how material affordances and constraints have the inherent capacity to suggest formal transformations. Digital fabrication typically excludes material agency because it requires the final form is digitally modelled before it can be fabricated. To enrich the fabrication design space with material agency, we introduce 1) a programming grammar that relates the sensing of material states with the transformation of fabrication actions via explicit rule notations; 2) a grammatical compiler that translates these rule notations into a responsive robot executable program; 3) a set of critical reflections on how this grammar enhances the fabrication design space with material agency. Consequently, our contributions broaden digital fabrication to produce intricate material forms that cannot be simulated by geometrical definitions.
ER  - 

TY  - JOUR
T1  - On the N3O2- paradigm
AU  - Alijah, Alexander
AU  - Kryachko, Eugene S.
JO  - Journal of Molecular Structure
VL  - 844-845
SP  - 193
EP  - 199
PY  - 2007
DA  - 2007/11/12/
T2  - STUDIES IN HYDROGEN-BONDED SYSTEMS – A collection of Invited Papers in honour of Professor Lucjan Sobcyk, on the occasion of his 80th Birthday
SN  - 0022-2860
DO  - https://doi.org/10.1016/j.molstruc.2007.04.024
UR  - https://www.sciencedirect.com/science/article/pii/S0022286007003316
KW  - 
KW  - Theoretical calculations
KW  - Isomers
KW  - Electron detachment
AB  - A survey of the existing experimental and theoretical data on the trinitrogen dioxide anion N3O2- that manifests a controversy as to the number of isomers and their chemical structures is presented. To resolve the controversy, new computational studies are performed at the MP2/aug-cc-pVTZ computational level. Two hitherto unknown isomers are predicted, one with singlet and one with triplet spin multiplicity. The singlet isomer, structurally characterized as N2·[ONO]−, is the most stable among all known isomers and accounts for fragmentation patterns observed in the recent dissociative photodetachment experiments.
ER  - 

TY  - JOUR
T1  - Sparse factors for the positive and negative syndrome scale: Which symptoms and stage of illness?
AU  - Anderson, Ariana
AU  - Wilcox, Marsha
AU  - Savitz, Adam
AU  - Chung, Hearee
AU  - Li, Qingqin
AU  - Salvadore, Giacomo
AU  - Wang, Dai
AU  - Nuamah, Isaac
AU  - Riese, Steven P.
AU  - Bilder, Robert M.
JO  - Psychiatry Research
VL  - 225
IS  - 3
SP  - 283
EP  - 290
PY  - 2015
DA  - 2015/02/28/
SN  - 0165-1781
DO  - https://doi.org/10.1016/j.psychres.2014.12.025
UR  - https://www.sciencedirect.com/science/article/pii/S016517811401018X
KW  - PANSS
KW  - Confirmatory factor analysis
KW  - Exploratory factor analysis
KW  - Schizophrenia
KW  - RDoC
KW  - Dimensional Measures
AB  - The Positive and Negative Syndrome Scale (PANSS) is frequently described with five latent factors, yet published factor models consistently fail to replicate across samples and related disorders. We hypothesize that (1) a subset of the PANSS, instead of the entire PANSS scale, would produce the most replicable five-factor models across samples, and that (2) the PANSS factor structure may be different depending on the treatment phase, influenced by the responsiveness of the positive symptoms to treatment. Using exploratory factor analysis, confirmatory factor analysis and cross validation on baseline and post-treatment observations from 3647 schizophrenia patients, we show that five-factor models fit best across samples when substantial subsets of the PANSS items are removed. The optimal model at baseline (five factors) omits 12 items: Motor Retardation, Grandiosity, Somatic Concern, Lack of Judgment and Insight, Difficulty in Abstract Thinking, Mannerisms and Posturing, Disturbance of Volition, Preoccupation, Disorientation, Excitement, Guilt Feelings and Depression. The PANSS factor models fit differently before and after patients have been treated. Patients with larger treatment response in positive symptoms have larger variations in factor structure across treatment stage than the less responsive patients. Negative symptom scores better predict the positive symptoms scores after treatment than before treatment. We conclude that sparse factor models replicate better on new samples, and the underlying disease structure of Schizophrenia changes upon treatment.
ER  - 

TY  - JOUR
T1  - Rapid dynamic changes of FL.2 variant: A case report of COVID-19 breakthrough infection
AU  - Choga, Wonderful T.
AU  - Kurusa (Gasenna), Gobuiwang Khilly
AU  - San, James Emmanuel
AU  - Ookame, Tidimalo
AU  - Gobe, Irene
AU  - Chand, Mohammed
AU  - Phafane, Badisa
AU  - Seru, Kedumetse
AU  - Matshosi, Patience
AU  - Zuze, Boitumelo
AU  - Ndlovu, Nokuthula
AU  - Matsuru, Teko
AU  - Maruapula, Dorcas
AU  - Bareng, Ontlametse T.
AU  - Macheke, Kutlo
AU  - Kuate-Lere, Lesego
AU  - Tlale, Labapotswe
AU  - Lesetedi, Onalethata
AU  - Tau, Modiri
AU  - Mbulawa, Mpaphi B.
AU  - Smith-Lawrence, Pamela
AU  - Matshaba, Mogomotsi
AU  - Shapiro, Roger
AU  - Makhema, Joseph
AU  - Martin, Darren P.
AU  - de Oliveira, Tulio
AU  - Lessells, Richard J.
AU  - Lockman, Shahin
AU  - Gaseitsiwe, Simani
AU  - Moyo, Sikhulile
JO  - International Journal of Infectious Diseases
VL  - 138
SP  - 91
EP  - 96
PY  - 2024
DA  - 2024/01/01/
SN  - 1201-9712
DO  - https://doi.org/10.1016/j.ijid.2023.11.011
UR  - https://www.sciencedirect.com/science/article/pii/S1201971223007725
KW  - SARS-CoV-2
KW  - Evolution
KW  - FL.2
KW  - Immunocompromised
KW  - Botswana
AB  - We investigated intra-host genetic evolution using two SARS-CoV-2 isolates from a fully vaccinated (primary schedule x2 doses of AstraZeneca plus a booster of Pfizer), >70-year-old woman with a history of lymphoma and hypertension who presented a SARS-CoV-2 infection for 3 weeks prior to death due to COVID-19. Two full genome sequences were determined from samples taken 13 days apart with both belonging to Pango lineage FL.2: the first detection of this Omicron sub-variant in Botswana. FL.2 is a sub-lineage of XBB.1.9.1. The repertoire of mutations and minority variants in the Spike protein differed between the two time points. Notably, we also observed deletions within the ORF1a and Membrane proteins; both regions are associated with high T-cell epitope density. The internal milieu of immune-suppressed individuals may accelerate SARS-CoV-2 evolution; hence, close monitoring is warranted.
ER  - 

TY  - JOUR
T1  - Variances in the impact of visual stimuli on design problem solving performance
AU  - Goldschmidt, Gabriela
AU  - Smolkov, Maria
JO  - Design Studies
VL  - 27
IS  - 5
SP  - 549
EP  - 569
PY  - 2006
DA  - 2006/09/01/
SN  - 0142-694X
DO  - https://doi.org/10.1016/j.destud.2006.01.002
UR  - https://www.sciencedirect.com/science/article/pii/S0142694X06000172
KW  - creativity
KW  - design problems
KW  - problem solving
KW  - visual stimuli
AB  - Research in cognitive psychology and in design thinking has shown that the generation of inner representations in imagery and external representations via sketching are instrumental in design problem solving. In this paper we focus on another facet of visual representation in design: the ‘consumption’ of external visual representations, regarded as stimuli, when those are present in the designer's work environment. An empirical study revealed that the presence of visual stimuli of different kinds can affect performance, measured in terms of practicality, originality and creativity scores attained by designs developed by subjects under different conditions. The findings suggest that the effect of stimuli is contingent on the type of the design problem that is being solved.
ER  - 

TY  - JOUR
T1  - Assessing the impact of standards-based grading policy changes on student performance and practice work completion in secondary mathematics
AU  - Huey, Maryann E.
AU  - Silvey, Patrick R.
AU  - Vaughan, Amy G.
AU  - Fisher, Asa L.
JO  - Studies in Educational Evaluation
VL  - 75
SP  - 101211
PY  - 2022
DA  - 2022/12/01/
SN  - 0191-491X
DO  - https://doi.org/10.1016/j.stueduc.2022.101211
UR  - https://www.sciencedirect.com/science/article/pii/S0191491X22000888
KW  - Grading
KW  - Standards-based grading
KW  - Mathematics
KW  - Secondary
KW  - Motivation
KW  - High-achieving students
AB  - We report upon an intervention study conducted over two academic calendar years involving high-achieving, grade 8 and 9 students (n = 122 and 123 respectively) enrolled in a year-long geometry course. The study assesses the impact of a change in grading policy, namely removing practice work from grade computations, on student performance levels and behaviors. After the change in grading policy was implemented, findings reveal that performance decreased on some, but not all, standards assessed. Completion rates of practice work also decreased overall. Potential causes are discussed as well as implications for implementing aspects of standards-based grading systems in secondary mathematics classrooms.
ER  - 

TY  - JOUR
T1  - MFGAD: Multi-fuzzy granules anomaly detection
AU  - Yuan, Zhong
AU  - Chen, Hongmei
AU  - Luo, Chuan
AU  - Peng, Dezhong
JO  - Information Fusion
VL  - 95
SP  - 17
EP  - 25
PY  - 2023
DA  - 2023/07/01/
SN  - 1566-2535
DO  - https://doi.org/10.1016/j.inffus.2023.02.007
UR  - https://www.sciencedirect.com/science/article/pii/S1566253523000490
KW  - Granular computing
KW  - Fuzzy rough set theory
KW  - Unsupervised anomaly detection
KW  - Multi-granularity
KW  - Hybrid data
AB  - Unsupervised anomaly detection is an important research direction in the process of unsupervised knowledge acquisition. It has been successfully applied in many fields, such as online fraud identification, loan approval, and medical diagnosis. Multi-granularity thinking is an effective information fusion method for solving problems in a multi-granular environment, which allows people to understand and analyze problems from multiple perspectives. However, there are few studies on building anomaly detection models using the idea of multi-fuzzy granules. To this end, this paper constructs a multi-fuzzy granules anomaly detection method by using a fuzzy rough computing model. In this method, a hybrid metric is first used to calculate the fuzzy relations. Then, two ranking sequences are constructed based on the significance of attributes. Furthermore, forward and reverse multi-fuzzy granules are constructed to define anomaly scores based on the ranking sequences. Finally, a multi-fuzzy granules-based anomaly detection algorithm is designed to detect anomalies. The experimental results compared with existing algorithms show the effectiveness of the proposed algorithm.
ER  - 

TY  - JOUR
T1  - Turing and von Neumann machines: Completing the new mechanism
AU  - Jurková, Barbora
AU  - Zámečník, Lukáš
JO  - Biosystems
VL  - 234
SP  - 105046
PY  - 2023
DA  - 2023/12/01/
SN  - 0303-2647
DO  - https://doi.org/10.1016/j.biosystems.2023.105046
UR  - https://www.sciencedirect.com/science/article/pii/S0303264723002216
KW  - Turing machine
KW  - von Neumann probe
KW  - New mechanism
KW  - Code biology
KW  - Extended mechanism
AB  - Turing (1937) introduces a model of code that is followed by other pioneers of computing machines (such as Flowers 1983, Eckert, Mauchly, Brainerd 1945 and others). One of them is John von Neumann, who defines the concept of optimal code in the context of the conception of EDVAC. He later uses it to build on in his theoretical considerations of the universal constructor (von Neumann 1966). Von Neumann (1963) further presents one of the first neural network models, in relation to the work of McCulloch and Pitts (1943), for both theoretical purposes (von Neumann probe) and practical applications (computer architecture of EDVAC). The aim of this paper is (1) to describe the differences between Turingʼs and von Neumannʼs conceptualizations of code and the mechanical computing model. Between von Neumann's abstract technical conception (von Neumann 1963 and 1966) and Turingʼs more concrete biochemical conception (Turing 1952). Furthermore, (2) we want to answer the question why these influential models of mechanisms (predominantly in computer science) have so far been ignored by philosophers of the new mechanism (Machamer, Darden, Craver 2000, Glennan 2017). We will show that these classical models of machines are not only compatible with the new mechanism, but moreover complement it, since they represent a completely separate type of model of mechanism, alongside producing, maintaining and underlying (Zámečník 2021). The final (3) and main goal of our paper will be an attempt to relate von Neumannʼs and Turingʼs notion of mechanism to Barbieriʼs notion of extended mechanism (Barbieri 2015).
ER  - 

TY  - JOUR
T1  - A hybrid decision support system for automatic detection of Schizophrenia using EEG signals
AU  - Khare, Smith K.
AU  - Bajaj, Varun
JO  - Computers in Biology and Medicine
VL  - 141
SP  - 105028
PY  - 2022
DA  - 2022/02/01/
SN  - 0010-4825
DO  - https://doi.org/10.1016/j.compbiomed.2021.105028
UR  - https://www.sciencedirect.com/science/article/pii/S0010482521008222
KW  - Schizophrenia
KW  - Electroencephalography
KW  - Optimization
KW  - Robust variational mode decomposition
KW  - Optimized extreme learning machine classifier
AB  - Background
Schizophrenia (SCZ) is a serious neurological condition in which people suffer with distorted perception of reality. SCZ may result in a combination of delusions, hallucinations, disordered thinking, and behavior. This causes permanent disability and hampers routine functioning. Trained neurologists use interviewing and visual inspection techniques for the detection and diagnosis of SCZ. These techniques are manual, time-consuming, subjective, and error-prone. Therefore, there is a need to develop an automatic model for SCZ classification. The aim of this study is to develop an automated SCZ classification model using electroencephalogram (EEG) signals. The EEG signals can capture the changes in neural dynamics of human cognition during SCZ.
Method
Based on the nature of the SCZ condition, the EEG signals must be examined. For accurate interpretation of EEG signals during SCZ, an automated model integrating a robust variational mode decomposition (RVMD) and an optimized extreme learning machine (OELM) classifier is developed. Traditional VMD suffers from noisy mode generation, mode duplication, under segmentation, and mode discarding. These problems are suppressed in RVMD by automating the selection of quadratic penalty factor (α) and a number of modes (L). The hyperparameters (HPM) of the OELM classifier are automatically selected to ensure maximum accuracy for each mode without overfitting or underfitting. For the selection of α and L in RVMD and HPM in the OELM classifier, a whale optimization algorithm is used. The root mean square error is minimized for RVMD and classification accuracy of each mode is maximized for the OELM classifier. The EEG signals of three conditions performing basic sensory tasks have been analyzed to detect SCZ.
Results
The Kruskal Wallis test is used to select different features extracted from the modes produced by RVMD. An OELM classifier is tested using a ten-fold cross-validation technique. An accuracy, precision, specificity, F-1 measure, sensitivity, and Cohen's Kappa of 92.93%, 93.94%, 91.06% 94.07%, 97.15%, and 85.32% are obtained.
Conclusion
The third mode's chaotic features helped to capture the significant changes that occurred during the SCZ state. The findings of the RVMD-OELM-based hybrid decision support system can help neuro-experts for the accurate identification of SCZ in real-time scenarios.
ER  - 

TY  - JOUR
T1  - Progressive expansion: Cost-efficient medical image analysis model with reversed once-for-all network training paradigm
AU  - Lim, Shin Wei
AU  - Chan, Chee Seng
AU  - Mohd Faizal, Erma Rahayu
AU  - Ewe, Kok Howg
JO  - Neurocomputing
VL  - 581
SP  - 127512
PY  - 2024
DA  - 2024/05/07/
SN  - 0925-2312
DO  - https://doi.org/10.1016/j.neucom.2024.127512
UR  - https://www.sciencedirect.com/science/article/pii/S0925231224002832
KW  - Medical image analysis
KW  - Machine learning
KW  - Model optimization
KW  - Cost-effective model
AB  - Low computational cost artificial intelligence (AI) models are vital in promoting the accessibility of real-time medical services in underdeveloped areas. The recent Once-For-All (OFA) network (without retraining) can directly produce a set of sub-network designs with Progressive Shrinking (PS) algorithm; however, the training resource and time inefficiency downfalls are apparent in this method. In this paper, we propose a new OFA training algorithm, namely the Progressive Expansion (ProX) to train the medical image analysis model. It is a reversed paradigm to PS, where technically we train the OFA network from the minimum configuration and gradually expand the training to support larger configurations. Empirical results showed that the proposed paradigm could reduce training time up to 68%; while still being able to produce sub-networks that have either similar or better accuracy compared to those trained with OFA-PS on ROCT (classification), BRATS and Hippocampus (3D-segmentation) public medical datasets. The code implementation for this paper is accessible at: https://github.com/shin-wl/ProX-OFA.
ER  - 

TY  - JOUR
T1  - Knowledge graph based reasoning in medical image analysis: A scoping review
AU  - Huang, Qinghua
AU  - Li, Guanghui
JO  - Computers in Biology and Medicine
VL  - 182
SP  - 109100
PY  - 2024
DA  - 2024/11/01/
SN  - 0010-4825
DO  - https://doi.org/10.1016/j.compbiomed.2024.109100
UR  - https://www.sciencedirect.com/science/article/pii/S0010482524011855
KW  - Medical diagnosis
KW  - Medical expert systems
KW  - Knowledge graph
KW  - Medical image analysis
AB  - Automated computer-aided diagnosis (CAD) is becoming more significant in the field of medicine due to advancements in computer hardware performance and the progress of artificial intelligence. The knowledge graph is a structure for visually representing knowledge facts. In the last decade, a large body of work based on knowledge graphs has effectively improved the organization and interpretability of large-scale complex knowledge. Introducing knowledge graph inference into CAD is a research direction with significant potential. In this review, we briefly review the basic principles and application methods of knowledge graphs firstly. Then, we systematically organize and analyze the research and application of knowledge graphs in medical imaging-assisted diagnosis. We also summarize the shortcomings of the current research, such as medical data barriers and deficiencies, low utilization of multimodal information, and weak interpretability. Finally, we propose future research directions with possibilities and potentials to address the shortcomings of current approaches.
ER  - 

TY  - JOUR
T1  - Metastability in Senescence
AU  - Naik, Shruti
AU  - Banerjee, Arpan
AU  - Bapi, Raju S.
AU  - Deco, Gustavo
AU  - Roy, Dipanjan
JO  - Trends in Cognitive Sciences
VL  - 21
IS  - 7
SP  - 509
EP  - 521
PY  - 2017
DA  - 2017/07/01/
SN  - 1364-6613
DO  - https://doi.org/10.1016/j.tics.2017.04.007
UR  - https://www.sciencedirect.com/science/article/pii/S1364661317300797
KW  - healthy aging
KW  - whole-brain computational modeling
KW  - metastability
AB  - The brain during healthy aging exhibits gradual deterioration of structure but maintains a high level of cognitive ability. These structural changes are often accompanied by reorganization of functional brain networks. Existing neurocognitive theories of aging have argued that such changes are either beneficial or detrimental. Despite numerous empirical investigations, the field lacks a coherent account of the dynamic processes that occur over our lifespan. Taking advantage of the recent developments in whole-brain computational modeling approaches, we hypothesize that the continuous process of aging can be explained by the concepts of metastability − a theoretical framework that gives a systematic account of the variability of the brain. This hypothesis can bridge the gap between existing theories and the empirical findings on age-related changes.
ER  - 

TY  - JOUR
T1  - Algorithmic adaptation and generalization of physically-constrained games
AU  - Uhlmann, Jeffrey
JO  - Entertainment Computing
VL  - 36
SP  - 100389
PY  - 2021
DA  - 2021/01/01/
SN  - 1875-9521
DO  - https://doi.org/10.1016/j.entcom.2020.100389
UR  - https://www.sciencedirect.com/science/article/pii/S1875952120300975
KW  - Aesthetic image transformations
KW  - Circular-shift operator
KW  - Computer games
KW  - Engineering education
KW  - Game development
KW  - Sliding-tile puzzles
KW  - Solitaire
KW  - TriPeaks
AB  - In this paper we introduce a novel strategy for generalizing existing puzzles and games by mathematically expressing the operations of the game; then deriving mathematical generalizations of those expressions; and finally implementing a new variant of the game using those generalized operations. The strategy is illustrated in a case study involving the adapting of a traditional game/puzzle to exploit the computational power of smart devices. The focus here is not so much on the end product as it is on the process and considerations underpinning its development by use of the proposed approach. Ancillary results of the venture include generalizations of the circular-shift operator and examination of its computational complexity.
ER  - 

TY  - JOUR
T1  - An efficient multilevel thresholding based satellite image segmentation approach using a new adaptive cuckoo search algorithm
AU  - Rahaman, Jarjish
AU  - Sing, Mihir
JO  - Expert Systems with Applications
VL  - 174
SP  - 114633
PY  - 2021
DA  - 2021/07/15/
SN  - 0957-4174
DO  - https://doi.org/10.1016/j.eswa.2021.114633
UR  - https://www.sciencedirect.com/science/article/pii/S0957417421000749
KW  - Thresholding
KW  - Segmentation
KW  - Otsu’s inter-class variance
KW  - Tsallis entropy
KW  - Cuckoo search algorithm
KW  - McCulloch’s method
KW  - A new adaptive cuckoo search algorithm
AB  - It is the most challenging and difficult task to segment a satellite image because of its complete randomness, multiple regions of interest, weak correlation with pixels, and regions of ambiguity. There are several Nature-inspired algorithms available, which are used to overcome these difficulties and those are more efficient to generate the best threshold value for the segmentation of satellite images. Though various modern methodologies opt for better results but methods have some drawbacks too like techniques are computationally expensive and time-consuming. In this paper, we have proposed a more effective satellite image segmentation approach using a new adaptive cuckoo search (ACS) algorithm. The result obtained from the projected technique is compared with CSMcCulloch incorporating McCulloch’s method for levy flight generation in Cuckoo Search (CS) algorithm by using two different objective functions namely Otsu’s method and Tsallis entropy function. The measurement techniques such as PSNR, MSE, FSIM, SSIM, UIQI, and computational time in term of CPU running time have been considered for validating and evaluating the proposed method. This proposed algorithm technique has resulted in improve segmentation quality of satellite images and reduced computational time. The analysis of the convergence rate proves that ACS is superior to the CSMcCulloch algorithm for reaching the global convergence rate. These experimental outcomes help to encourage researchers in different domains such as computer vision, application of medical image analysis, machine learning as well as deep learning.
ER  - 

TY  - JOUR
T1  - Digital ecosystem: The journey of a metaphor
AU  - Krivý, Maroš
JO  - Digital Geography and Society
VL  - 5
SP  - 100057
PY  - 2023
DA  - 2023/12/01/
SN  - 2666-3783
DO  - https://doi.org/10.1016/j.diggeo.2023.100057
UR  - https://www.sciencedirect.com/science/article/pii/S2666378323000090
KW  - Nature
KW  - Ecosystem
KW  - Digital capitalism
KW  - Platform capitalism
KW  - Metaphor
KW  - Imaginaries
AB  - The term “digital ecosystem” has become ubiquitous through a seemingly endless stream of scholarship, punditry and hyperbole around digitalization, to the point that the metaphor is becoming dead. Considering “ecosystem” as a traveling concept straddling natural, social and technical systems, this article traces the extension of “digital ecosystem,” along with the adjacent “business ecosystem” and “entrepreneurial ecosystem,” in the fields of computer science, economy, governance and environmental policy. The origins of the concept as a form of circuitry applied to nature are outlined as a background against which to trace its role as a socio-technical metaphor for digital capitalism. Since the 1990s, various formulations of “ecosystem” have offered a naturalistic interpretation to phenomena ranging from economic interactions to digital infrastructure and the urban everyday. I conclude that by representing the internet and the market as complex, self-organizing processes, the metaphor prioritizes the imperative of adapting to—and downplays the possibility of challenging—our erratic digital capitalism. The article contributes by illuminating the ideological work of naturalistic models in the digital political economy. Evidence on using digital ecosystems in environmental policy is still emerging but points to a form of legitimacy exchange that reduces environmental problems to technical issues.
ER  - 

TY  - JOUR
T1  - Tools from evolutionary biology shed new light on the diversification of languages
AU  - Levinson, Stephen C.
AU  - Gray, Russell D.
JO  - Trends in Cognitive Sciences
VL  - 16
IS  - 3
SP  - 167
EP  - 173
PY  - 2012
DA  - 2012/03/01/
SN  - 1364-6613
DO  - https://doi.org/10.1016/j.tics.2012.01.007
UR  - https://www.sciencedirect.com/science/article/pii/S1364661312000290
AB  - Computational methods have revolutionized evolutionary biology. In this paper we explore the impact these methods are now having on our understanding of the forces that both affect the diversification of human languages and shape human cognition. We show how these methods can illuminate problems ranging from the nature of constraints on linguistic variation to the role that social processes play in determining the rate of linguistic change. Throughout the paper we argue that the cognitive sciences should move away from an idealized model of human cognition, to a more biologically realistic model where variation is central.
ER  - 

TY  - JOUR
T1  - On the semantics for spreadsheets with sheet-defined functions
AU  - Bock, Alexander Asp
AU  - Bøgholm, Thomas
AU  - Sestoft, Peter
AU  - Thomsen, Bent
AU  - Thomsen, Lone Leth
JO  - Journal of Computer Languages
VL  - 57
SP  - 100960
PY  - 2020
DA  - 2020/04/01/
SN  - 2590-1184
DO  - https://doi.org/10.1016/j.cola.2020.100960
UR  - https://www.sciencedirect.com/science/article/pii/S2590118420300204
KW  - Spreadsheet
KW  - Semantics
KW  - Funcalc
KW  - Sheet-defined function
KW  - Recalculation
AB  - We give an operational semantics for the evaluation of spreadsheets, including sheet-defined and built-in numeric functions in the Funcalc spreadsheet platform. The semantics allows for different implementations and we discuss sheet-defined functions implemented using both interpretation and run-time code generation. The semantics specifies the expected result of a computation, also considering non-deterministic functions, independently of an evaluation mechanism. It can be extended to include the cost of formula evaluation for a cost analysis e.g. for use in parallelization of computations. An interesting future direction is to investigate experimentally how close our semantics is to that of major spreadsheet implementations.
ER  - 

TY  - JOUR
T1  - An overview on the roles of fuzzy set techniques in big data processing: Trends, challenges and opportunities
AU  - Wang, Hai
AU  - Xu, Zeshui
AU  - Pedrycz, Witold
JO  - Knowledge-Based Systems
VL  - 118
SP  - 15
EP  - 30
PY  - 2017
DA  - 2017/02/15/
SN  - 0950-7051
DO  - https://doi.org/10.1016/j.knosys.2016.11.008
UR  - https://www.sciencedirect.com/science/article/pii/S0950705116304452
KW  - Big data
KW  - Data-intensive science
KW  - Fuzzy sets
KW  - Fuzzy logic
KW  - Granular computing
AB  - In the era of big data, we are facing with an immense volume and high velocity of data with complex structures. Data can be produced by online and offline transactions, social networks, sensors and through our daily life activities. A proper processing of big data can result in informative, intelligent and relevant decision making completed in various areas, such as medical and healthcare, business, management and government. To handle big data more efficiently, new research paradigm has been engaged but the ways of thinking about big data call for further long-term innovative pursuits. Fuzzy sets have been employed for big data processing due to their abilities to represent and quantify aspects of uncertainty. Several innovative approaches within the framework of Granular Computing have been proposed. To summarize the current contributions and present an outlook of further developments, this overview addresses three aspects: (1) We review the recent studies from two distinct views. The first point of view focuses on what types of fuzzy set techniques have been adopted. It identifies clear trends as to the usage of fuzzy sets in big data processing. Another viewpoint focuses on the explanation of the benefits of fuzzy sets in big data problems. We analyze when and why fuzzy sets work in these problems. (2) We present a critical review of the existing problems and discuss the current challenges of big data, which could be potentially and partially solved in the framework of fuzzy sets. (3) Based on some principles, we infer the possible trends of using fuzzy sets in big data processing. We stress that some more sophisticated augmentations of fuzzy sets and their integrations with other tools could offer a novel promising processing environment.
ER  - 

TY  - CHAP
T1  - Architecture
AU  - Goldschmidt, G.
A2  - Runco, Mark A.
A2  - Pritzker, Steven R.
BT  - Encyclopedia of Creativity (Second Edition)
PB  - Academic Press
CY  - San Diego
SP  - 46
EP  - 51
PY  - 2011
DA  - 2011/01/01/
SN  - 978-0-12-375038-9
DO  - https://doi.org/10.1016/B978-0-12-375038-9.00010-8
UR  - https://www.sciencedirect.com/science/article/pii/B9780123750389000108
KW  - Architectural design
KW  - Architectural education
KW  - Culture
KW  - Digital design
KW  - Form
KW  - Function
KW  - Ideas
KW  - Leading idea
KW  - Starchitect
KW  - Style
AB  - Architecture is a cultural arena based on ideas, which communally produce styles and individually, at their best, generate outstanding buildings. Every building tackles form and function. In our era architecture is expected to innovate in its forms, while ensuring perfect functionality. Form and function handling are rough correlates of originality and practicality, by which we measure design creativity. Architecture is also a product of the technological state of its time. At present we experience computational advances that promise to fundamentally change buildings and the manner in which they are designed. Architectural education is groping to adjust to the changes.
ER  - 

TY  - JOUR
T1  - Study of gender perspective in STEM degrees and its relationship with video games
AU  - Lavalle, Ana
AU  - Teruel, Miguel A.
AU  - Maté, Alejandro
AU  - Trujillo, Juan
JO  - Entertainment Computing
SP  - 100889
PY  - 2024
DA  - 2024/09/19/
SN  - 1875-9521
DO  - https://doi.org/10.1016/j.entcom.2024.100889
UR  - https://www.sciencedirect.com/science/article/pii/S187595212400257X
KW  - Gender studies
KW  - Games
KW  - Cultural and social implications
AB  - At present, even though gender equality is an important matter of public interest, there are still areas in higher education where male presence is overwhelming. We refer to STEM (Science, Technology, Engineering, and Mathematics) studies in general and Computer Engineering in particular where there is only 16% of female presence in Spain. This fact made us think about the reason for this inequality. We attempted to answer such questions by means of a survey filled out by 138 students of STEM university degrees. Thanks to this survey, we have been able to understand the motivations and opinions of the students of STEM degrees regarding gender perspective. Our study highlights the possible influence of computer and video game use on enrollment in STEM degrees. Furthermore, it points out the differences between men and women in computer science skills before they start their studies. The answers provided by the surveyed women showed a correlation between women who play video games and those who get better grades. In addition, women who play video games feel more integrated into STEM degrees. Finally, differences were found in gender perspective between the male and female participants.
ER  - 

TY  - JOUR
T1  - The mediating effects of coping strategies in the relationship between automatic negative thoughts and depression in a clinical sample of diabetes patients
AU  - Clarke, Dave
AU  - Goosen, Tanya
JO  - Personality and Individual Differences
VL  - 46
IS  - 4
SP  - 460
EP  - 464
PY  - 2009
DA  - 2009/03/01/
SN  - 0191-8869
DO  - https://doi.org/10.1016/j.paid.2008.11.014
UR  - https://www.sciencedirect.com/science/article/pii/S0191886908004285
KW  - Automatic thoughts
KW  - Cognitive behaviour therapy
KW  - Coping
KW  - Depression
KW  - Diabetes
AB  - High levels of depression have been found among diabetes patients, but few studies have examined the influence of coping strategies on the relationship between diabetics’ negative thoughts and their depression. The purpose of the study was to investigate the effects of coping strategies as mediators in the path from automatic negative thoughts to depression. A questionnaire containing the Automatic Thoughts Questionnaire, the Ways of Coping Checklist, a depression inventory and demographic questions was completed by 57 male and 57 female New Zealand diabetic patients, aged 28–88 years (median=60.5, mean=59.3, SD=14.6). Automatic negative thoughts, emotion-focused coping and depression, but not problem-focused coping, were significantly correlated, after controlling for relevant demographic and diabetes variables. Hierarchical linear regression analysis of data showed that emotion-focused coping functioned as a partial mediator between negative thoughts and depression. Cognitive therapy was suggested to control both automatic negative thoughts and emotion-focused coping behaviours of self-blame, wishful thinking and avoidance.
ER  - 

TY  - JOUR
T1  - Secret image sharing using grayscale payload decomposition and irreversible image steganography
AU  - Chakraborty, Soumendu
AU  - Jalal, Anand Singh
AU  - Bhatnagar, Charul
JO  - Journal of Information Security and Applications
VL  - 18
IS  - 4
SP  - 180
EP  - 192
PY  - 2013
DA  - 2013/12/01/
SN  - 2214-2126
DO  - https://doi.org/10.1016/j.istr.2013.02.006
UR  - https://www.sciencedirect.com/science/article/pii/S1363412713000162
KW  - DSF matrix
KW  - Error matrix
KW  - Sign matrix
KW  - Bit plane
AB  - To provide an added security level most of the existing reversible as well as irreversible image steganography schemes emphasize on encrypting the secret image (payload) before embedding it to the cover image. The complexity of encryption for a large payload where the embedding algorithm itself is complex may adversely affect the steganographic system. Schemes that can induce same level of distortion, as any standard encryption technique with lower computational complexity, can improve the performance of stego systems. In this paper, we propose a secure secret image sharing scheme, which bears minimal computational complexity. The proposed scheme, as a replacement for encryption, diversifies the payload into different matrices which are embedded into carrier image (cover image) using bit X-OR operation. A payload is a grayscale image which is divided into frequency matrix, error matrix, and sign matrix. The frequency matrix is scaled down using a mapping algorithm to produce Down Scaled Frequency (DSF) matrix. The DSF matrix, error matrix, and sign matrix are then embedded in different cover images using bit X-OR operation between the bit planes of the matrices and respective cover images. Analysis of the proposed scheme shows that it effectively camouflages the payload with minimum computation time.
ER  - 

TY  - JOUR
T1  - Cyber-physical systems research and education in 2030: Scenarios and strategies
AU  - Gürdür Broo, Didem
AU  - Boman, Ulf
AU  - Törngren, Martin
JO  - Journal of Industrial Information Integration
VL  - 21
SP  - 100192
PY  - 2021
DA  - 2021/03/01/
SN  - 2452-414X
DO  - https://doi.org/10.1016/j.jii.2020.100192
UR  - https://www.sciencedirect.com/science/article/pii/S2452414X20300674
KW  - Cyber-physical systems
KW  - Research and education
KW  - 2030
KW  - Future studies
KW  - Scenario planning TAIDA framework
KW  - Future of engineering research
KW  - Future of research
AB  - Future cyber-physical systems (CPS), such as smart cities, collaborative robots, autonomous vehicles or intelligent transport systems, are expected to be highly intelligent, electrified, and connected. This study explores a focal question about how these new characteristics may affect the education and research related to CPS in 2030, the date identified by the United Nations to achieve the Agenda for Sustainable Development. To this end, first, we have conducted a trend spotting activity, seeking to identify possible influencing factors that may have a great impact on the future of CPS education and research. These factors were clustered in a total of 12 trends – four certainties; namely connectivity, electrification, data and automation – and eight uncertainties; namely intelligence, data ethics, labour market, lifelong learning, higher education, trust in technology, technological development speed, and sustainable development goals. After that, two of the eight uncertainties are identified and used to construct a scenario matrix, which includes four scenarios. These two uncertainties – the so-called strategic uncertainties – are: fulfilment of sustainable development goals and the nature of the technological development, respectively. These two important uncertainties are considered to build the scenarios due to their potential impact on the research and education of CPS. For instance, sustainable development goals are significant targets for many initiatives, organisations and countries. While 2030 is the deadline to achieve these goals, the relationship between the sustainable development goals related to CPS research and education is not studied well. Similarly, the speed of technological development is seen as a driving force behind future CPS. However, the effect of this speed to CPS research and education environment is not known. Different outcomes of the chosen two uncertainties are, then, combined with the remaining trends and uncertainties. Consequently, four scenarios are derived. The Terminator scenario illustrates a dystopian future where profit is the driving force behind technological progress and sustainable development goals are not accomplished. In contrast, The Iron Giant scenario represents the successful implementation of the sustainable development goals where technological development is the force behind the accomplishment of these goals. The scenario called Slow Progress represents a future where gradual technological improvements are present, but sustainability is still not seen as concerning the issue. The Humanist scenario illustrates a future where slow technological development is happening yet sustainable development goals are successfully implemented. Finally, the scenarios are used to initiate discussions by illustrating what the future of research and education could look like and a list of strategies for future CPS research and education environments is proposed. To this end, we invite educators, researchers, institutions and governments to develop the necessary strategies to enable data-orientated, continuous, interdisciplinary, collaborative, ethical, and sustainable research and education by improving digital fluency, advancing digital equality, contributing to new ways of teaching complex thinking, expanding access to learning platforms and preparing next generations to adapt for a rapidly changing future of work conditions.
ER  - 

TY  - JOUR
T1  - Energy aware edge computing: A survey
AU  - Jiang, Congfeng
AU  - Fan, Tiantian
AU  - Gao, Honghao
AU  - Shi, Weisong
AU  - Liu, Liangkai
AU  - Cérin, Christophe
AU  - Wan, Jian
JO  - Computer Communications
VL  - 151
SP  - 556
EP  - 580
PY  - 2020
DA  - 2020/02/01/
SN  - 0140-3664
DO  - https://doi.org/10.1016/j.comcom.2020.01.004
UR  - https://www.sciencedirect.com/science/article/pii/S014036641930831X
KW  - Edge computing
KW  - Energy efficiency
KW  - Computing offloading
KW  - Benchmarking
KW  - Computation partitioning
AB  - Edge computing is an emerging paradigm for the increasing computing and networking demands from end devices to smart things. Edge computing allows the computation to be offloaded from the cloud data centers to the network edge and edge nodes for lower latency, security and privacy preservation. Although energy efficiency in cloud data centers has been broadly investigated, energy efficiency in edge computing is largely left uninvestigated due to the complicated interactions between edge devices, edge servers, and cloud data centers. In order to achieve energy efficiency in edge computing, a systematic review on energy efficiency of edge devices, edge servers, and cloud data centers is required. In this paper, we survey the state-of-the-art research work on energy-aware edge computing, and identify related research challenges and directions, including architecture, operating system, middleware, applications services, and computation offloading.
ER  - 

TY  - JOUR
T1  - DAMQT 2.0: A new version of the DAMQT package for the analysis of electron density in molecules
AU  - López, Rafael
AU  - Rico, Jaime Fernández
AU  - Ramírez, Guillermo
AU  - Ema, Ignacio
AU  - Zorrilla, David
JO  - Computer Physics Communications
VL  - 192
SP  - 289
EP  - 294
PY  - 2015
DA  - 2015/07/01/
SN  - 0010-4655
DO  - https://doi.org/10.1016/j.cpc.2015.02.027
UR  - https://www.sciencedirect.com/science/article/pii/S0010465515000855
KW  - Electron density
KW  - Electrostatic potential
KW  - Electric field
KW  - Hellmann–Feynman forces
KW  - Density deformations
AB  - DAMQT 2.0 is a new version of the DAMQT package for the analysis of electron density in molecules and the fast computation of the density, density deformations, electrostatic potential and field, and Hellmann–Feynman forces. Algorithms for the partition of the electron density and the computation of related properties like density deformations, electrostatic potential and field and Hellmann–Feynman forces have been improved and their codes, fully rewritten. MPI versions of the most computational demanding modules are now included in the package for parallel computation. The Graphical User Interface has been also enhanced, with new features including a 2D plotter and significant improvements in the 3D viewer.
Program summary
Program title: DAMQT 2.0 Catalogue identifier: AEDL_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEDL_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: GPLv3 No. of lines in distributed program, including test data, etc.: 317,270 No. of bytes in distributed program, including test data, etc.: 40,193,220 Distribution format: tar.gz Programming language: Fortran90 and C++. Computer: Any. Operating system: Linux, Windows (7, 8). RAM: 200 Mbytes Classification: 16.1. Catalogue identifier of previous version: AEDL_v1_0 Journal reference of previous version: Comput. Phys. Comm. 180(2009)1654 External routines: Qt (4.8 or higher), OpenGL (3.x or higher), freeGLUT 2.8.x Nature of problem: Analysis of the molecular electron density and density deformations, including fast evaluation of electrostatic potential, electric field and Hellmann–Feynman forces on nuclei. Solution method: The method of Deformed Atoms in Molecules, reported elsewhere [1], is used for partitioning the molecular electron density into atomic fragments, which are further expanded in spherical harmonics times radial factors. The partition is used for defining molecular density deformations and for the fast calculation of several properties associated to density. Restrictions: Density must come from an LCAO calculation (any level) with spherical (not Cartesian) Slater or Gaussian functions. Unusual features: The program contains an OPEN statement to binary files (stream) in several files. This statement has not a standard syntax in Fortran 90. Two possibilities are considered in conditional compilation: Intel’s ifort and Fortran2003 standard. The latter is applied to compilers other than ifort (gfortran uses this one, for instance). Additional comments: Quick-start guide and User’s manual in PDF format included in the package. User’s manual is also accessible from the Graphical User Interface. The distribution file for this program is over 40 Mbytes and therefore is not delivered directly when downloaded or Email is requested. Instead an html file giving details of how the program can be obtained is sent. Running time: Largely dependent on the system size and the module run (from fractions of a second to hours). References:[1]J. Fernández Rico, R. López, I. Ema and G. Ramírez, J. Mol. Struct. Theochem 727 (2005) 115.
ER  - 

TY  - CHAP
T1  - Chapter 24 - The brain-machine interface, nanosensor technology, and artificial intelligence: Their convergence with a novel frontier
AU  - Kalnoor, Gauri
A2  - Hussain, Chaudhery Mustansar
A2  - Kailasa, Suresh Kumar
BT  - Handbook of Nanomaterials for Sensing Applications
PB  - Elsevier
SP  - 575
EP  - 587
PY  - 2021
DA  - 2021/01/01/
T2  - Micro and Nano Technologies
SN  - 978-0-12-820783-3
DO  - https://doi.org/10.1016/B978-0-12-820783-3.00013-0
UR  - https://www.sciencedirect.com/science/article/pii/B9780128207833000130
KW  - Neuroscience
KW  - Machine learning
KW  - Nanotechnology
KW  - Artificial intelligence (AI)
KW  - Brain-computer interface
KW  - Brain-machine interface (BMI)
KW  - Computational neuroscience
AB  - A confluence of technological capabilities is creating an opportunity for machine learning and artificial intelligence (AI) to enable “smart” nanoengineered brain-machine interfaces (BMI). This new generation of technologies will be able to communicate with the brain in ways that support contextual learning and adaptation to change functional requirements. This applies to both invasive technologies aimed at restoring neurological function, as in the case of neural prosthesis, as well as noninvasive technologies enabled by signals such as electroencephalograph (EEG). Advances in computation, hardware, and algorithms that learn and adapt in a contextually dependent way will be able to leverage the capabilities that nanoengineering offers the design and functionality of BMI. We explore the enabling capabilities that these devices may exhibit, why they matter, and the state of the technologies necessary to build them. We also discuss a number of open technical challenges and problems that will need to be solved to achieve this.
ER  - 

TY  - CHAP
T1  - Past, present, and future of educational measurement
AU  - Clauser, Brian E.
AU  - Margolis, Melissa J.
A2  - Tierney, Robert J
A2  - Rizvi, Fazal
A2  - Ercikan, Kadriye
BT  - International Encyclopedia of Education (Fourth Edition)
PB  - Elsevier
CY  - Oxford
SP  - 1
EP  - 14
PY  - 2023
DA  - 2023/01/01/
SN  - 978-0-12-818629-9
DO  - https://doi.org/10.1016/B978-0-12-818630-5.10001-6
UR  - https://www.sciencedirect.com/science/article/pii/B9780128186305100016
KW  - Karl Pearson
KW  - Francis Galton
KW  - Classical test theory
KW  - Item response theory
KW  - Generalizability theory
KW  - Frederic Lord
KW  - Lee Cronbach
KW  - Coefficient alpha
KW  - Validity theory
KW  - Charles Spearman
KW  - Eugenics movement
KW  - Spearman-Brown formula
KW  - Alfred Binet
KW  - Army Alpha test
KW  - Georg Rasch
KW  - Intelligence testing
AB  - This article provides an overview of the past, present, and future of educational measurement. We begin by examining the historical events in the 1800s that led to the development of a coherent mathematical theory of test scores in the first half of the 20th century. In this section we describe the contributions of Francis Galton, Karl Pearson, Charles Spearman, Truman Kelley, and Lee Cronbach. In addition to outlining the theoretical contributions of these researchers, we describe the rise of large-scale testing beginning with the Army Alpha test in 1917 and the administration of IQ tests to millions of school children in the decade that followed. We continue by discussing the current state of educational measurement theory and practice including the development and widespread use of item response theory, generalizability theory, validity theory, and large-scale national and international achievement testing to evaluate educational systems. Finally, we consider directions and developments that are likely to define the future of the field. These directions include increased use of computational power in assessment, the use of new sources of data (referred to as process data), automated systems to create test materials, and an increased emphasis on fairness.
ER  - 

TY  - JOUR
T1  - An intention inference method for the space non-cooperative target based on BiGRU-Self Attention
AU  - Zhang, Honglin
AU  - Luo, Jianjun
AU  - Gao, Yuan
AU  - Ma, Weihua
JO  - Advances in Space Research
VL  - 72
IS  - 5
SP  - 1815
EP  - 1828
PY  - 2023
DA  - 2023/09/01/
SN  - 0273-1177
DO  - https://doi.org/10.1016/j.asr.2023.04.032
UR  - https://www.sciencedirect.com/science/article/pii/S0273117723003101
KW  - Space non-cooperative target
KW  - Intention inference
KW  - Time series
KW  - BiGRU
KW  - Self-attention mechanism
AB  - Intention inference for space non-cooperative targets is the key to space situational awareness and assistant decision for collision avoidance. Given that the problem of target intention inference is essential to learn the dynamically changing time-series characteristics of space non-cooperative target intentions and infer their relative motion patterns for threat warning, this paper adopts a deep learning-based approach, introduces a bidirectional propagation mechanism and self-attention mechanism based on Gated Recurrent Unit (GRU) and proposes a bidirectional Gated Recurrent Unit (BiGRU)-Self Attention-based space non-cooperative target intention inference model. BiGRU is used to learn deep information in time-series characteristics of the space non-cooperative target, and self-attention mechanism is used to adaptively extract and assign weights to key characteristics to capture the internal correlations in time-series information, thus improving model performance. The line-of-sight measurements are used as the characteristics of target intention inference, and the typical target motion intentions are defined. Subsequently, the proposed model is trained and tested on the test set, with the accuracy reaching 97.1%. Besides, the effectiveness and advantages of the proposed model are verified by the simulation of a case study and comparison evaluations. The results demonstrate that our proposed model could significantly improve the accuracy, computational efficiency, and noise resistance for the space non-cooperative target intention inference compared with the existing intention inference models.
ER  - 

TY  - JOUR
T1  - Approximate methods for optimal replacement, maintenance, and inspection policies
AU  - Zhao, Xufeng
AU  - Al-Khalifa, Khalifa N.
AU  - Nakagawa, Toshio
JO  - Reliability Engineering & System Safety
VL  - 144
SP  - 68
EP  - 73
PY  - 2015
DA  - 2015/12/01/
SN  - 0951-8320
DO  - https://doi.org/10.1016/j.ress.2015.07.005
UR  - https://www.sciencedirect.com/science/article/pii/S0951832015001957
KW  - Hazard function
KW  - Mean time to failure
KW  - Age replacement
KW  - Imperfect maintenance
KW  - Approximate inspection
AB  - It might be difficult sometimes to derive theoretical and numerical solutions for analytical maintenance modelings due to the computational complexity. This paper takes up several approximate models in maintenance theory, by using the cumulative hazard function H(t) and the newly proposed asymptotic MTTF (Mean Time to Failure) skilfully. We firstly denote by tx the time when the expected number of failures is x. Using H(tx)=x, we estimate failure times, model age and periodic replacements, and sequential imperfect maintenance. Motivated by the asymptotic method of computation of MTTF, we secondly model the expected cost rate for a parallel system when replacement is made at system failure, and give approximate computations for the sequential inspection policy. Optimizations of each model are obtained approximately in an easier way. When failure times have a Weibull distribution, it is shown from numerical examples that the obtained approximate optimal solutions have good approximations of the exact ones.
ER  - 

TY  - JOUR
T1  - Self-referential basis of undecidable dynamics: From the Liar paradox and the halting problem to the edge of chaos
AU  - Prokopenko, Mikhail
AU  - Harré, Michael
AU  - Lizier, Joseph
AU  - Boschetti, Fabio
AU  - Peppas, Pavlos
AU  - Kauffman, Stuart
JO  - Physics of Life Reviews
VL  - 31
SP  - 134
EP  - 156
PY  - 2019
DA  - 2019/12/01/
T2  - Physics of Mind
SN  - 1571-0645
DO  - https://doi.org/10.1016/j.plrev.2018.12.003
UR  - https://www.sciencedirect.com/science/article/pii/S1571064519300077
KW  - Self-reference
KW  - Diagonalization
KW  - Undecidability
KW  - Incomputability
KW  - Program-data duality
KW  - Complexity
AB  - In this paper we explore several fundamental relations between formal systems, algorithms, and dynamical systems, focussing on the roles of undecidability, universality, diagonalization, and self-reference in each of these computational frameworks. Some of these interconnections are well-known, while some are clarified in this study as a result of a fine-grained comparison between recursive formal systems, Turing machines, and Cellular Automata (CAs). In particular, we elaborate on the diagonalization argument applied to distributed computation carried out by CAs, illustrating the key elements of Gödel's proof for CAs. The comparative analysis emphasizes three factors which underlie the capacity to generate undecidable dynamics within the examined computational frameworks: (i) the program-data duality; (ii) the potential to access an infinite computational medium; and (iii) the ability to implement negation. The considered adaptations of Gödel's proof distinguish between computational universality and undecidability, and show how the diagonalization argument exploits, on several levels, the self-referential basis of undecidability.
ER  - 

TY  - JOUR
T1  - SCIIENCE: The creation and pilot implementation of an NGSS-based instrument to evaluate early childhood science teaching
AU  - Kaderavek, Joan N.
AU  - North, Tamala
AU  - Rotshtein, Regina
AU  - Dao, Hoangha
AU  - Liber, Nicholas
AU  - Milewski, Geoff
AU  - Molitor, Scott C.
AU  - Czerniak, Charlene M.
JO  - Studies in Educational Evaluation
VL  - 45
SP  - 27
EP  - 36
PY  - 2015
DA  - 2015/06/01/
SN  - 0191-491X
DO  - https://doi.org/10.1016/j.stueduc.2015.03.003
UR  - https://www.sciencedirect.com/science/article/pii/S0191491X15000218
KW  - Discourse analysis
KW  - Teacher assessment
KW  - Language of science in classrooms
KW  - Validity/reliability
AB  - This paper describes the development, testing and implementation of the Systematic Characterization of Inquiry Instruction in Early LearNing Classroom Environments (SCIIENCE). The SCIIENCE instrument was designed to capture best practices outlined in the National Research Council's Framework for K-12 Science Education as they occur within a science lesson. The goals of the SCIIENCE instrument are to (a) assess the quality of science instruction in PK-3 classrooms, (b) capture teacher behaviors and instructional practices that engage students in the lesson, promote scientific studies, encourage higher-level thinking, and (c) provide a feedback mechanism for guiding professional development of PK-3 teachers. Science educators can apply this instrument to teacher behaviors and use the data to improve classroom inquiry instructional methodology.
ER  - 

TY  - JOUR
T1  - Influence of curriculum reform: An analysis of student mathematics achievement in Mainland China
AU  - Ni, Yujing
AU  - Li, Qiong
AU  - Li, Xiaoqing
AU  - Zhang, Zhong-Hua
JO  - International Journal of Educational Research
VL  - 50
IS  - 2
SP  - 100
EP  - 116
PY  - 2011
DA  - 2011/01/01/
T2  - Curricular effect on the teaching and learning of mathematics: Findings from two longitudinal studies in China and the United States
SN  - 0883-0355
DO  - https://doi.org/10.1016/j.ijer.2011.06.005
UR  - https://www.sciencedirect.com/science/article/pii/S0883035511000413
KW  - Curriculum reform
KW  - Primary mathematics
KW  - Curriculum evaluation
KW  - Student mathematics achievement
KW  - Cognitive
KW  - Affective
AB  - This study investigated curriculum influences on student mathematics achievement by following two groups of students from fifth to sixth grade that were taught either the reformed curriculum or the conventional curriculum. Analyses with three-level modeling were conducted to examine learning outcomes of the students who were assessed three times over a period of 18 months. Achievement was measured with regard to computation, routine problem solving, and complex problem solving. Affective aspects included self-reported interest in learning mathematics, classroom participation, views of the nature of mathematics, and views of learning mathematics. The results showed overall improved performance among all the students over the time on computation, routine problem solving, and complex problem solving but not on the affective measures. There were differentiated patterns of performance between the groups. On the initial assessment, the reform group performed better than the non-reform group on calculation, complex problem solving, and indicated higher interest in learning mathematics. The two groups did not differ on the other achievement and affective measures at the first time of assessment. There was no significant difference in growth rate between the groups on the cognitive and affective measures except that the non-reform group progressed at a faster pace on calculation. Therefore, the non-reform group outperformed the reform group on computation at the third (last) assessment. These results are discussed with respect to the possible influence of the curriculum on student learning.
ER  - 

TY  - JOUR
T1  - Comparison of Effectiveness of Logistic Regression, Naive Bayes, and Random Forest Algorithms in Predicting Student Arguments
AU  - Wahyuningsih, Tri
AU  - Manongga, Danny
AU  - Sembiring, Irwan
AU  - Wijono, Sutarto
JO  - Procedia Computer Science
VL  - 234
SP  - 349
EP  - 356
PY  - 2024
DA  - 2024/01/01/
T2  - Seventh Information Systems International Conference (ISICO 2023)
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2024.03.014
UR  - https://www.sciencedirect.com/science/article/pii/S1877050924003715
KW  - Comparison Algorithm
KW  - Logistic Regression
KW  - Naive Bayes
KW  - Random Forest
KW  - Student Arguments
AB  - Currently, in the process of assessing and giving feedback on students' argumentative writing, educators have to spend a considerable amount of time reading and analyzing each essay individually. This can be a complicated and time-consuming process, especially if the number of students to be assessed is quite large. The problem of this research is to find the most effective algorithm in providing accurate and reliable predictions in the context of evaluation and feedback of students' argumentation. This study compares three algorithms (logistic regression, Naive Bayes, and Random Forest) to predict student argumentation using essays from grades 6-12. Logistic regression performed best with 94.34% accuracy, followed by random forest with 91.98% accuracy, and Naive Bayes with 88.93% accuracy. The study optimized preprocessing and selected algorithms for an automated guidance model. It is the first stage of a three-part study for developing automated guidance models. Data came from Kaggle, and the study aims to improve the accuracy of automated guidance models for student argumentation.
ER  - 

TY  - JOUR
T1  - The form of a ‘half-baked’ creative idea: Empirical explorations into the structure of ill-defined mental representations
AU  - Scotney, Victoria S.
AU  - Schwartz, Jasmine
AU  - Carbert, Nicole
AU  - Saab, Adam
AU  - Gabora, Liane
JO  - Acta Psychologica
VL  - 203
SP  - 102981
PY  - 2020
DA  - 2020/02/01/
SN  - 0001-6918
DO  - https://doi.org/10.1016/j.actpsy.2019.102981
UR  - https://www.sciencedirect.com/science/article/pii/S0001691819303129
KW  - Analogy
KW  - Art
KW  - Creative process
KW  - Honing
KW  - Mental representation
KW  - Structure mapping
AB  - Creative thought is conventionally believed to involve searching memory and generating multiple independent candidate ideas followed by selection and refinement of the most promising. Honing theory, which grew out of the quantum approach to describing how concepts interact, posits that what appears to be discrete, separate ideas are actually different projections of the same underlying mental representation, which can be described as a superposition state, and which may take different outward forms when reflected upon from different perspectives. As creative thought proceeds, this representation loses potentiality to be viewed from different perspectives and manifest as different outcomes. Honing theory yields different predictions from conventional theories about the mental representation of an idea midway through the creative process. These predictions were pitted against one another in two studies: one closed-ended and one open-ended. In the first study, participants were interrupted midway through solving an analogy problem and wrote down what they were thinking in terms of a solution. In the second, participants were instructed to create a painting that expressed their true essence and describe how they conceived of the painting. For both studies, naïve judges categorized these responses as supportive of either the conventional view or the honing theory view. The results of both studies were significantly more consistent with the predictions of honing theory. Some implications for creative cognition, and cognition in general, are discussed.
ER  - 

TY  - JOUR
T1  - Adaptive chaotic map-based key extraction for efficient cross-layer authentication in VANETs
AU  - Shawky, Mahmoud A.
AU  - Usman, Muhammad
AU  - Imran, Muhammad Ali
AU  - Abbasi, Qammer H.
AU  - Ansari, Shuja
AU  - Taha, Ahmad
JO  - Vehicular Communications
VL  - 39
SP  - 100547
PY  - 2023
DA  - 2023/02/01/
SN  - 2214-2096
DO  - https://doi.org/10.1016/j.vehcom.2022.100547
UR  - https://www.sciencedirect.com/science/article/pii/S2214209622000948
KW  - Chebyshev chaotic mapping
KW  - Cross-layer authentication
KW  - Doppler emulation
KW  - Physical-layer signatures
KW  - Secret key extraction
KW  - Vehicular ad-hoc networks
AB  - Vehicle-to-everything (V2X) communication is expected to offer users available and ultra-reliable transmission, particularly for critical applications related to safety and autonomy. In this context, establishing a secure and resilient authentication process with low latency and high functionality may not be achieved using conventional cryptographic methodologies due to their significant computation costs. Recent research has focused on employing the physical (PHY) characteristics of wireless channels to develop efficient discrimination techniques to overcome the shortcomings of crypto-based authentication. This paper presents a cross-layer authentication scheme for multicarrier communication, leveraging the spatially/temporally correlated wireless channel features to facilitate key verification without exposing its secrecy. By mapping the time-stamped hashed key and masking it with channel phase responses, we create a PHY-layer signature, allowing for verifying the sender's identity while employing the correlated channel responses between subcarriers to verify messages' integrity. Furthermore, we developed a Diffie-Hellman secret key extraction algorithm that employs the computationally intractable problems of the Chebyshev chaotic mapping for channel probing. Thus, terminals can extract high entropy shared keys that can be used to create dynamic PHY-layer signatures, supporting forward and backward secrecy. We evaluated the scheme's security strength against active/passive attacks. Besides theoretical analysis, we designed a 3-Dimensional (3D) scattering Doppler emulator to investigate the scheme's performance at different speeds of a moving vehicle and signal-to-noise ratios (SNRs) for a realistic vehicular channel. Theoretical and hardware implementation analyses proved the capability of the proposed scheme to support high detection probability at SNR ≥ 0 dB and speed ≤ 45 m/s.
ER  - 

TY  - CHAP
T1  - 17 - Risk assessment and management of civil infrastructure networks: a systems approach
AU  - Sánchez-Silva, M.
AU  - Gómez, C.
A2  - Tesfamariam, S.
A2  - Goda, K.
BT  - Handbook of Seismic Risk Analysis and Management of Civil Infrastructure Systems
PB  - Woodhead Publishing
SP  - 437
EP  - 464
PY  - 2013
DA  - 2013/01/01/
T2  - Woodhead Publishing Series in Civil and Structural Engineering
SN  - 978-0-85709-268-7
DO  - https://doi.org/10.1533/9780857098986.4.437
UR  - https://www.sciencedirect.com/science/article/pii/B9780857092687500177
KW  - infrastructure
KW  - transportation networks
KW  - systems thinking
KW  - risk assessment
KW  - decision-making
KW  - optimization
AB  - Abstract:
Infrastructure networks are complex systems due to the large number of components that interact in a nonlinear way. Detecting and understanding the properties of such systems is of paramount importance to make effective decisions about risk management and sustainable development. This chapter presents a systems approach to risk management and risk-based decision making in infrastructure networks. In the proposed approach, the internal structure of a network is detected via pattern recognition (clustering), and structured information is used to enhance conceptual and computational analyses of reliability, vulnerability, damage propagation, and resource allocation. The approach can be applied to network analysis of complex infrastructure systems subjected to extreme events, such as earthquakes.
ER  - 

TY  - JOUR
T1  - Time for change: Learning from community forests to enhance the resilience of multi-value forestry in British Columbia, Canada
AU  - Devisscher, Tahia
AU  - Spies, Jillian
AU  - Griess, Verena C.
JO  - Land Use Policy
VL  - 103
SP  - 105317
PY  - 2021
DA  - 2021/04/01/
SN  - 0264-8377
DO  - https://doi.org/10.1016/j.landusepol.2021.105317
UR  - https://www.sciencedirect.com/science/article/pii/S0264837721000405
KW  - Climate change
KW  - Community forestry
KW  - Resilience
KW  - Wildfire
KW  - Social license
KW  - Land tenure
AB  - Forests around the world are experiencing the cumulative effects of rapid social and environmental change. Building resilience in the forestry sector has thus become of major importance in many countries, including Canada. While British Columbia (BC) generates the highest revenue from the forestry sector in Canada, the planning and management of forests in this province face several limitations that hinder the application of resilience thinking in a fully integrated way that accounts not only for ecosystem processes but also the close interconnection between forests and people. Community forestry in BC provides experience gained over 20 years that can form the basis for a more holistic, long-term approach to enhance the resilience of forested landscapes. Based on interviews with managers of 5 case study community forests (CFs), and a survey of all CFs in BC over three consecutive years, we present pilot practices to manage forests for resilience at the stand- and landscape-levels. Findings show that these practices mainly focus on (1) age and species diversification, (2) introduction of more drought-tolerant species, (3) systematic long-term monitoring of productivity and forest health, (4) wildfire risk management, and (5) introduction of enhanced silviculture such as thinning, rehabilitation and fertilization. Between 2016 and 2018, 38 CFs in BC invested more than CAD 4.5 million in enhanced silvicultural practices using their own funds. The area-based tenure of CFs motivated not only long-term planning and investment, but also shifted the mindset among residents towards a more multi-functional and dynamic view of the forest. Building adaptive capacity and social license, CFs foster a future where forest health and community well-being are compatible. These lessons can be scaled to BC and other forested landscapes in Canada and around the world. Scaling mechanisms include: (1) facilitating knowledge exchange; (2) increasing multi-stakeholder collaboration; (3) replication and mainstreaming of effective practices; (4) rethinking the forest tenure system; and (5) systematic research and monitoring to learn from pilot studies that could inform strategic interventions with landscape-scale impact. Multi-functional forests which are increasingly affected by climate change and novel disturbances could particularly benefit from the insights shared in this paper to build social-ecological resilience.
ER  - 

TY  - JOUR
T1  - Generative urban design: A systematic review on problem formulation, design generation, and decision-making
AU  - Jiang, Feifeng
AU  - Ma, Jun
AU  - Webster, Christopher John
AU  - Chiaradia, Alain J.F.
AU  - Zhou, Yulun
AU  - Zhao, Zhan
AU  - Zhang, Xiaohu
JO  - Progress in Planning
VL  - 180
SP  - 100795
PY  - 2024
DA  - 2024/02/01/
T2  - Generative urban design: A systematic review on problem formulation, design generation, and decision-making
SN  - 0305-9006
DO  - https://doi.org/10.1016/j.progress.2023.100795
UR  - https://www.sciencedirect.com/science/article/pii/S0305900623000569
KW  - Generative urban design
KW  - Urban form generation
KW  - Generative method
KW  - AI-generated content (AIGC)
KW  - Generative AI
KW  - Human-machine collaboration
AB  - Urban design is the process of designing and shaping the physical forms of cities, towns, and suburbs. It involves the arrangement and design of street systems, groups of buildings, public spaces, and landscapes, to make the urban environment performative and sustainable. The typical design process, reliant on manual work and expert experience has unavoidable low efficiency in generating high-performing design solutions due to the involvement of complex social, institutional, and economic contexts and the trade-off between conflicting preferences of different stakeholder groups. Taking advantage of artificial intelligence (AI) and computational capacity, generative urban design (GUD) has been developed as a trending technical direction to narrow the gaps and produce design solutions with high efficiency at early design stages. It uses computer-aided generative methods, such as evolutionary optimization and deep generative models, to efficiently explore complex solution spaces and automatically generate design options that satisfy conflicting objectives and various constraints. GUD experiments have attracted much attention from academia, practitioners, and public authorities in recent years. However, a systematic review of the current stage of GUD research is lacking. This study, therefore, reports on a systematic investigation of the existing literature according to the three key stages in the GUD process: (1) design problem formulation, (2) design option generation, and (3) decision-making. For each stage, current trends, findings, and limitations from GUD studies are examined. Future directions and potential challenges are discussed and presented. The review is highly interdisciplinary and involves articles from urban study, computer science, social science, management, and other fields. It reports what scholars have found in GUD experiments and organizes a diverse and complicated technical agenda into something accessible to all stakeholders. The results and discoveries will serve as a holistic reference for GUD developers and users in both academia and industry and form a baseline for the field of GUD development in the coming years.
ER  - 

TY  - JOUR
T1  - A Dynamical Systems Perspective on Flexible Motor Timing
AU  - Remington, Evan D.
AU  - Egger, Seth W.
AU  - Narain, Devika
AU  - Wang, Jing
AU  - Jazayeri, Mehrdad
JO  - Trends in Cognitive Sciences
VL  - 22
IS  - 10
SP  - 938
EP  - 952
PY  - 2018
DA  - 2018/10/01/
T2  - Special Issue: Time in the Brain
SN  - 1364-6613
DO  - https://doi.org/10.1016/j.tics.2018.07.010
UR  - https://www.sciencedirect.com/science/article/pii/S1364661318301724
KW  - dynamical systems
KW  - flexible timing
KW  - sensorimotor control
KW  - learning
KW  - movement sequences
KW  - movement planning
AB  - A hallmark of higher brain function is the ability to rapidly and flexibly adjust behavioral responses based on internal and external cues. Here, we examine the computational principles that allow decisions and actions to unfold flexibly in time. We adopt a dynamical systems perspective and outline how temporal flexibility in such a system can be achieved through manipulations of inputs and initial conditions. We then review evidence from experiments in nonhuman primates that support this interpretation. Finally, we explore the broader utility and limitations of the dynamical systems perspective as a general framework for addressing open questions related to the temporal control of movements, as well as in the domains of learning and sequence generation.
ER  - 

TY  - JOUR
T1  - Undergraduates’ metacognitive knowledge about the psychological effects of different kinds of computer-supported instructional tools
AU  - Antonietti, Alessandro
AU  - Colombo, Barbara
AU  - Lozotsev, Yuri
JO  - Computers in Human Behavior
VL  - 24
IS  - 5
SP  - 2172
EP  - 2198
PY  - 2008
DA  - 2008/09/01/
T2  - Including the Special Issue: Internet Empowerment
SN  - 0747-5632
DO  - https://doi.org/10.1016/j.chb.2007.10.004
UR  - https://www.sciencedirect.com/science/article/pii/S0747563207001598
KW  - Metacognition
KW  - Belief
KW  - Learning
KW  - Education
KW  - Computer
AB  - Literature about metacognition suggests that learners develop personal beliefs about the educational technologies that they are asked to employ and that such beliefs can influence learning outcomes. In this perspective, opinions about the psychological effects of computer-supported instructional tools were analysed by means of a questionnaire which included items about the motivational and emotional aspects of learning, the behaviour to have during the learning process, the mental abilities and the style of thinking required, and the cognitive benefits. Items were presented five times: each time they made reference to a different kind of tool (online courses, hypertexts, Web forums, multimedia presentations, and virtual simulations). The questionnaire was filled out by 99 undergraduates attending engineering courses. Results showed that students ranked the psychological effects of the computer-supported tools in a relative different order according to the kind of tool and attributed distinctive effects to each tool. Gender and expertise played a minor role in modulating undergraduates’ beliefs. Implications for instruction were discussed.
ER  - 

TY  - JOUR
T1  - “Principles of Mechanics that are Susceptible of Application to Society”: An unpublished notebook of Adolphe Quetelet at the root of his social physics
AU  - Aubin, David
JO  - Historia Mathematica
VL  - 41
IS  - 2
SP  - 204
EP  - 223
PY  - 2014
DA  - 2014/05/01/
SN  - 0315-0860
DO  - https://doi.org/10.1016/j.hm.2014.01.001
UR  - https://www.sciencedirect.com/science/article/pii/S0315086014000020
KW  - Mechanics
KW  - Sociology
KW  - Adolphe Quetelet
KW  - Astronomy
KW  - Social physics
KW  - Average man
KW  - Applications of mathematics
KW  - Analogical thinking
AB  - Founder of the Brussels Observatory, Adolphe Quetelet (1796–1874) is especially well known for his theory of the average man. Like the average position of a star obtained through a large quantity of observed data, the average man was, according to Quetelet, subject to fixed causal laws. Published in 1835, his book On Man: Essay of Social Physics is one of the founding works of sociology and mathematical statistics. The sources of the analogy between astronomy and social physics have been debated by historians. To shed light on this question and the conditions of application of mathematics in the 19th century, we publish for the first time a manuscript that is kept in Quetelet's papers at the Royal Academy of Belgium, and give an English translation of it.
ER  - 
